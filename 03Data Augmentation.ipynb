{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0faa4981",
   "metadata": {},
   "source": [
    "## 03 数据增强"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedbe0e1",
   "metadata": {},
   "source": [
    "在上一个 Notebook 中，我们分析了数据集的基本情况，发现数据集中存在明显的不平衡现象，尤其是中性情感样本数量较少。为了缓解这种不平衡性对模型训练的影响，我们将对中性情感样本进行数据增强处理。\n",
    "\n",
    "下面介绍两种 NLP 任务中常用的数据增强方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e3b6d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment_type\n",
      "积极    167778\n",
      "消极    124334\n",
      "中立     66472\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "sentiment_type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "sentiment_polarity",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "d921ed8f-d835-4dd3-b21c-d86f83654f5e",
       "rows": [
        [
         "256351",
         "每次打完底在定妆之前都有一步重要工作 从脸上往下摘猫毛",
         "中立",
         "0"
        ],
        [
         "151206",
         "来买饮料的路上就这么水灵灵地又买上了",
         "中立",
         "0"
        ],
        [
         "284991",
         "连续三天吃喝tims",
         "中立",
         "0"
        ],
        [
         "47061",
         "9点多睡到11点多 然后看文到现在 就是不咋困",
         "中立",
         "0"
        ],
        [
         "102841",
         "为四点以后的生活 保持50的分享欲",
         "中立",
         "0"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment_type</th>\n",
       "      <th>sentiment_polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>256351</th>\n",
       "      <td>每次打完底在定妆之前都有一步重要工作 从脸上往下摘猫毛</td>\n",
       "      <td>中立</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151206</th>\n",
       "      <td>来买饮料的路上就这么水灵灵地又买上了</td>\n",
       "      <td>中立</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284991</th>\n",
       "      <td>连续三天吃喝tims</td>\n",
       "      <td>中立</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47061</th>\n",
       "      <td>9点多睡到11点多 然后看文到现在 就是不咋困</td>\n",
       "      <td>中立</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102841</th>\n",
       "      <td>为四点以后的生活 保持50的分享欲</td>\n",
       "      <td>中立</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               text sentiment_type  sentiment_polarity\n",
       "256351  每次打完底在定妆之前都有一步重要工作 从脸上往下摘猫毛             中立                   0\n",
       "151206           来买饮料的路上就这么水灵灵地又买上了             中立                   0\n",
       "284991                   连续三天吃喝tims             中立                   0\n",
       "47061       9点多睡到11点多 然后看文到现在 就是不咋困             中立                   0\n",
       "102841            为四点以后的生活 保持50的分享欲             中立                   0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "# 读取数据\n",
    "df = pd.read_csv('data/weibo_features.csv', encoding='utf-8-sig')\n",
    "\n",
    "# 进行至此，可以删除掉 DataFrame 中的冗余字段，如文本长度 text_length\n",
    "df = df.drop(columns=[\"text_length\"])\n",
    "\n",
    "print(df[\"sentiment_type\"].value_counts())\n",
    "\n",
    "neutral_data = df[df[\"sentiment_type\"] == \"中立\"]\n",
    "\n",
    "neutral_data.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aabb751",
   "metadata": {},
   "source": [
    "### 1. 回译（Back-translation）\n",
    "回译是一种常用的数据增强方法，通过将文本翻译成另一种语言再翻译回来，从而生成新的文本。这个过程可以增加数据的多样性，帮助模型更好地泛化。\n",
    "\n",
    "例如：\n",
    "\n",
    "- 原始文本：今天北京温度在 20 度左右。\n",
    "- 翻译成英文：The temperature in Beijing is around 20°C today.\n",
    "- 再翻译回中文：北京今天气温大约二十摄氏度。\n",
    "\n",
    "通过回译，我们可以生成与原始文本意思相近但表述不同的新样本。生成风格自然的新增句子。\n",
    "\n",
    "这里，我们使用 HuggingFace 上的 Helsinki-NLP/opus-mt-zh-en 和 Helsinki-NLP/opus-mt-en-zh 翻译模型完成回译任务。这两个模型是 OPUS-MT(Open Parallel Corpus - Machine Translation) 项目的重要组成部分，专门针对中英互译任务进行了深度优化。\n",
    "\n",
    "由于模型位于 Huggingface 服务器上，若通过国内网络访问模型来处理数据，可能会受限于网络状况导致效果不佳。因此，我们先将模型从 Huggingface 下载到本地，然后再加载本地模型进行离线翻译。\n",
    "\n",
    "两个翻译模型已事先下载至项目根目录下的 models 文件夹中，见 `models/opus-mt-zh-en` 和 `models/opus-mt-en-zh`。\n",
    "\n",
    "```python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d50d971",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "def load_model(src, tgt):\n",
    "    model_name = f'models/opus-mt-{src}-{tgt}'\n",
    "    tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "    model = MarianMTModel.from_pretrained(model_name)\n",
    "    return model, tokenizer\n",
    "\n",
    "def translate(texts, model, tokenizer):\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    outputs = model.generate(**inputs)\n",
    "    translated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return translated_texts\n",
    "\n",
    "def back_translate(sentence):\n",
    "    en = translate([sentence], model_zh2en, tok_zh2en)[0]\n",
    "    zh = translate([en], model_en2zh, tok_en2zh)[0]\n",
    "    return zh\n",
    "\n",
    "model_zh2en, tok_zh2en = load_model('zh', 'en')\n",
    "model_en2zh, tok_en2zh = load_model('en', 'zh')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b069f62a",
   "metadata": {},
   "source": [
    "我们从中性情感样本中抽取$50\\%$的数据进行回译增强，生成新的中性样本。得到的增强样本数量为\n",
    "$$66472 \\times 50\\% = 33236$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b012a508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['你们俩 谈恋爱不准耽误学习！',\n",
       " '真假？不是说深圳是最后的行程吗',\n",
       " '黄礼志，像翻盖手机一样的女人。',\n",
       " '2025年',\n",
       " '一夜花开',\n",
       " 'hola！',\n",
       " '深呼吸深呼吸',\n",
       " '抖抖发了12条，其他的草稿发在微博不过分吧',\n",
       " '心理治疗的本质，不过是一种自律的工具。 斯科特派克少有人走的路',\n",
       " '往事有底片为证']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_to_translate = neutral_data[\"text\"].sample(frac=0.5, random_state=42).to_list()\n",
    "\n",
    "texts_to_translate[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724b3ce4",
   "metadata": {},
   "source": [
    "为了提高回译效率，我们使用批量回译的方法处理数据。\n",
    "\n",
    "简单的实验表明，批量回译相较于逐条回译，效率可提升约 2 倍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83198487",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def batch_back_translate(sentences, saved_path=\"data/texts_back_translated.csv\", \n",
    "                                   batch_size=32, max_length=512, save_every=100):\n",
    "    \"\"\"\n",
    "    增量保存版本的批量回译函数\n",
    "    \n",
    "    Args:\n",
    "        sentences: 要翻译的句子列表\n",
    "        saved_path: CSV保存路径\n",
    "        batch_size: 批处理大小\n",
    "        max_length: 最大序列长度\n",
    "        save_every: 每处理多少条数据保存一次\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame包含所有回译结果\n",
    "    \"\"\"\n",
    "    # 检查是否已有部分结果\n",
    "    if os.path.exists(saved_path):\n",
    "        existing_df = pd.read_csv(saved_path, encoding='utf-8-sig')\n",
    "        processed_count = len(existing_df)\n",
    "        print(f\"发现已处理 {processed_count} 条数据，从第 {processed_count + 1} 条开始...\")\n",
    "        sentences = sentences[processed_count:]\n",
    "        if not sentences:\n",
    "            print(\"所有数据已处理完成！\")\n",
    "            return existing_df\n",
    "    else:\n",
    "        existing_df = pd.DataFrame(columns=['original_text', 'augmented_text', 'intermediate_english'])\n",
    "        processed_count = 0\n",
    "    \n",
    "    # 创建保存目录\n",
    "    os.makedirs(os.path.dirname(saved_path), exist_ok=True)\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    print(f\"开始处理剩余 {len(sentences)} 条文本...\")\n",
    "    \n",
    "    # 分批处理\n",
    "    for i in tqdm(range(0, len(sentences), batch_size), desc=\"增量回译中\"):\n",
    "        batch = sentences[i:i + batch_size]\n",
    "        \n",
    "        try:\n",
    "            # 执行回译\n",
    "            inputs_zh = tok_zh2en(batch, return_tensors=\"pt\", padding=True, \n",
    "                                 truncation=True, max_length=max_length)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                en_outputs = model_zh2en.generate(**inputs_zh, max_length=max_length, \n",
    "                                                num_beams=4, early_stopping=True)\n",
    "                en_texts = tok_zh2en.batch_decode(en_outputs, skip_special_tokens=True)\n",
    "            \n",
    "            inputs_en = tok_en2zh(en_texts, return_tensors=\"pt\", padding=True, \n",
    "                                 truncation=True, max_length=max_length)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                zh_outputs = model_en2zh.generate(**inputs_en, max_length=max_length,\n",
    "                                                num_beams=4, early_stopping=True)\n",
    "                zh_texts = tok_en2zh.batch_decode(zh_outputs, skip_special_tokens=True)\n",
    "            \n",
    "            # 收集批次结果\n",
    "            batch_results = []\n",
    "            for j, (original, translated, english) in enumerate(zip(batch, zh_texts, en_texts)):\n",
    "                batch_results.append({\n",
    "                    'original_text': original,\n",
    "                    'augmented_text': translated,\n",
    "                    'intermediate_english': english\n",
    "                })\n",
    "            \n",
    "            all_results.extend(batch_results)\n",
    "            \n",
    "            # 定期保存结果\n",
    "            if len(all_results) >= save_every or i + batch_size >= len(sentences):\n",
    "                # 合并现有数据和新数据\n",
    "                new_df = pd.DataFrame(all_results)\n",
    "                combined_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "                \n",
    "                # 保存到文件\n",
    "                combined_df.to_csv(saved_path, index=False, encoding='utf-8-sig')\n",
    "                \n",
    "                print(f\"已保存 {len(combined_df)} 条数据到 {saved_path}\")\n",
    "                \n",
    "                # 更新现有数据框\n",
    "                existing_df = combined_df\n",
    "                all_results = []  # 清空临时结果\n",
    "            \n",
    "            # 清理GPU内存\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"处理批次时出错: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"所有数据处理完成！最终结果: {len(existing_df)} 条\")\n",
    "    return existing_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "011c5a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "发现已处理 33236 条数据，从第 33237 条开始...\n",
      "所有数据已处理完成！\n",
      "回译结果大小：33236\n",
      "删除缺失值后回译结果大小：33234\n"
     ]
    }
   ],
   "source": [
    "translated_results = batch_back_translate(texts_to_translate)\n",
    "print(f\"回译结果大小：{len(translated_results)}\")\n",
    "translated_results = translated_results.dropna()\n",
    "print(f\"删除缺失值后回译结果大小：{len(translated_results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cab1d9",
   "metadata": {},
   "source": [
    "从抽取的回译结果中可以看到，微博作为社交平台，用户发布的内容往往较为口语化，往往并不严格按照书面语的规范组织文字，这导致了回译生成的文本可能出现语义偏差、结构混乱、信息缺失等问题。\n",
    "\n",
    "然而，尽管回译生成的文本在语义上存在细微偏差，但在情感倾向上与原文本基本保持一致，因此，这些回译生成的文本仍然可以作为有效的中性情感样本用于数据增强，帮助模型更好地学习中性情感的表达方式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c7546c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "回译前数据集中性样本数量: 66472\n",
      "回译后数据集中性样本数量: 99706\n"
     ]
    }
   ],
   "source": [
    "translated_data = DataFrame([\n",
    "    (text, \"中立\", 0) for text in translated_results[\"augmented_text\"]\n",
    "], columns=[\"text\", \"sentiment_type\", \"sentiment_polarity\"])\n",
    "\n",
    "# 将回译数据加入到原始 DataFrame 中\n",
    "df_aug = pd.concat([df, translated_data], ignore_index=True)\n",
    "    \n",
    "# 打乱\n",
    "df_aug = df_aug.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "print(f\"回译前数据集中性样本数量: {len(neutral_data)}\")\n",
    "print(f\"回译后数据集中性样本数量: {len(df_aug[df_aug['sentiment_type'] == '中立'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296b6532",
   "metadata": {},
   "source": [
    "最后，我们查看数据增强后数据集中标签的分布情况："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bfd1e0f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment_type\n",
      "积极    167778\n",
      "消极    124334\n",
      "中立     99706\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "type_counts = df_aug[\"sentiment_type\"].value_counts()\n",
    "print(type_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdaf6585",
   "metadata": {},
   "source": [
    "和 `02Feature Engineering and Data Analysis.ipynb` 一样，计算最大类/最小类 比率 IR 与变异系数 CV：\n",
    "> - $IR \\leq 1.5$ → 非常平衡\n",
    "> - $1.5 < IR \\leq 3$ → 轻度不平衡\n",
    "> - $IR > 3$ → 明显不平衡\n",
    "> - $IR > 10$ → 高度不平衡，需要采样或加权"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c68ab72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据增强前 IR = 2.52\n",
      "数据增强后 IR = 1.68\n"
     ]
    }
   ],
   "source": [
    "type_counts_before = df[\"sentiment_type\"].value_counts()\n",
    "\n",
    "ir_before = type_counts_before.max() / type_counts_before.min()\n",
    "ir = type_counts.max() / type_counts.min()\n",
    "\n",
    "print(f\"数据增强前 IR = {ir_before:.2f}\")\n",
    "print(f\"数据增强后 IR = {ir:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef58715e",
   "metadata": {},
   "source": [
    "从 IR 可以看出不平衡程度有所减小。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d82bb1",
   "metadata": {},
   "source": [
    "> - $CV \\leq 0.1$ → 非常平衡\n",
    "> - $0.1 < CV \\leq 0.3$ → 可接受\n",
    "> - $CV > 0.3$ → 明显不平衡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "866c7cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据增强前 CV = 0.43\n",
      "数据增强后 CV = 0.26\n"
     ]
    }
   ],
   "source": [
    "cv_before = type_counts_before.std() / type_counts_before.mean()\n",
    "cv = type_counts.std() / type_counts.mean()\n",
    "\n",
    "print(f\"数据增强前 CV = {cv_before:.2f}\")\n",
    "print(f\"数据增强后 CV = {cv:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3d66bc",
   "metadata": {},
   "source": [
    "从 CV 同样可以看到不平衡程度有所减弱。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a717a3f3",
   "metadata": {},
   "source": [
    "在这个 Notebook 中，我们主要为了缓解数据集中的类别不平衡问题，采用了回译的方法对中性情感样本进行了数据增强。通过回译生成了大量新的中性样本，丰富了数据集的多样性，有助于提升模型在中性情感识别上的表现。\n",
    "\n",
    "然而，正如 IR 和 CV 值所示，数据集仍然存在一定程度的不平衡问题。在后续的模型训练过程中，我们将考虑结合其他技术手段，如加权损失函数，进一步缓解类别不平衡对模型性能的影响。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c611b2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aug.to_csv('data/weibo_augmented.csv', index=False, encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpllma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
