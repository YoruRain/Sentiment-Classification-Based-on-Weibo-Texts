{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0faa4981",
   "metadata": {},
   "source": [
    "# 03 数据增强\n",
    "\n",
    "本 Notebook 中，我们将对整理好的文本情感分类数据集，采用“回译”的方法进行数据增强处理，以提升模型的泛化能力和鲁棒性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedbe0e1",
   "metadata": {},
   "source": [
    "## 3.1 背景提要\n",
    "\n",
    "在上一个 Notebook 中，我们分析了数据集的基本情况，发现数据集中存在明显的不平衡现象，尤其是中性情感样本数量较少。\n",
    "\n",
    "为了缓解这种不平衡性对模型训练的影响，我们将对中性情感样本进行数据增强处理。\n",
    "\n",
    "接下来，我们介绍一种 NLP 任务中常用的数据增强方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3b6d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment_type\n",
      "积极    167778\n",
      "消极    124334\n",
      "中立     66472\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "sentiment_type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "sentiment_polarity",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "310b5d5e-d00d-4dd8-a89c-c8c81b8ad40e",
       "rows": [
        [
         "220040",
         "如何知道自己食量变大 之前醒来第一顿两个四半贝果吃不完 现在两个吃不饱 （中午吃饱了的前提下）",
         "中立",
         "0"
        ],
        [
         "54208",
         "没有答案也是一种答案",
         "中立",
         "0"
        ],
        [
         "263246",
         "打个卡222公里。 漳州云霄",
         "中立",
         "0"
        ],
        [
         "1504",
         "去打工了 八九点钟的太阳",
         "中立",
         "0"
        ],
        [
         "314764",
         "对已经免疫了 挨十几针毫无感觉了已经",
         "中立",
         "0"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment_type</th>\n",
       "      <th>sentiment_polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>220040</th>\n",
       "      <td>如何知道自己食量变大 之前醒来第一顿两个四半贝果吃不完 现在两个吃不饱 （中午吃饱了的前提下）</td>\n",
       "      <td>中立</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54208</th>\n",
       "      <td>没有答案也是一种答案</td>\n",
       "      <td>中立</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263246</th>\n",
       "      <td>打个卡222公里。 漳州云霄</td>\n",
       "      <td>中立</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1504</th>\n",
       "      <td>去打工了 八九点钟的太阳</td>\n",
       "      <td>中立</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314764</th>\n",
       "      <td>对已经免疫了 挨十几针毫无感觉了已经</td>\n",
       "      <td>中立</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text sentiment_type  \\\n",
       "220040  如何知道自己食量变大 之前醒来第一顿两个四半贝果吃不完 现在两个吃不饱 （中午吃饱了的前提下）             中立   \n",
       "54208                                        没有答案也是一种答案             中立   \n",
       "263246                                   打个卡222公里。 漳州云霄             中立   \n",
       "1504                                       去打工了 八九点钟的太阳             中立   \n",
       "314764                               对已经免疫了 挨十几针毫无感觉了已经             中立   \n",
       "\n",
       "        sentiment_polarity  \n",
       "220040                   0  \n",
       "54208                    0  \n",
       "263246                   0  \n",
       "1504                     0  \n",
       "314764                   0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "read_path = 'data/preparation/weibo_features.csv'\n",
    "\n",
    "# 读取数据\n",
    "df = pd.read_csv(read_path, encoding='utf-8-sig')\n",
    "\n",
    "# 进行至此，可以删除掉 DataFrame 中的冗余字段，如文本长度 text_length\n",
    "df = df.drop(columns=[\"text_length\"])\n",
    "\n",
    "print(df[\"sentiment_type\"].value_counts())\n",
    "\n",
    "neutral_data = df[df[\"sentiment_type\"] == \"中立\"]\n",
    "\n",
    "neutral_data.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aabb751",
   "metadata": {},
   "source": [
    "## 3.2 数据增强方法：回译\n",
    "\n",
    "### 3.2.1 回译方法简介\n",
    "\n",
    "回译（Back-translation）是自然语言处理任务中一种常用的数据增强方法，通过 **将文本翻译成另一种语言再翻译回来**，从而生成新的文本。\n",
    "\n",
    "这个过程可以增加数据的多样性，帮助模型更好地泛化。\n",
    "\n",
    "例如：\n",
    "\n",
    "- 原始文本：今天北京温度在 20 度左右。\n",
    "- 翻译成英文：The temperature in Beijing is around 20°C today.\n",
    "- 再翻译回中文：北京今天气温大约二十摄氏度。\n",
    "\n",
    "通过回译，我们可以生成与原始文本意思相近但表述不同的新样本。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab39985",
   "metadata": {},
   "source": [
    "### 3.2.2 翻译模型介绍\n",
    "\n",
    "这里，我们使用 HuggingFace 上的 *Helsinki-NLP/opus-mt-zh-en* 和 *Helsinki-NLP/opus-mt-en-zh* 翻译模型完成回译任务。\n",
    "\n",
    "这两个模型是 OPUS-MT(Open Parallel Corpus - Machine Translation) 项目的重要组成部分，专门针对中英互译任务进行了深度优化。\n",
    "\n",
    "由于模型位于 Huggingface 服务器上，若通过国内网络访问模型来处理数据，可能会受限于网络状况导致效果不佳。\n",
    "\n",
    "因此，我们先将模型从 Huggingface 下载到本地，然后再加载本地模型进行离线翻译。\n",
    "\n",
    "两个翻译模型已事先下载至项目根目录下的 models 文件夹中，见 `models/opus-mt-zh-en` 和 `models/opus-mt-en-zh`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d50d971",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "def load_model(src, tgt):\n",
    "    model_name = f'models/opus-mt-{src}-{tgt}'\n",
    "    tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "    model = MarianMTModel.from_pretrained(model_name)\n",
    "    return model, tokenizer\n",
    "\n",
    "def translate(texts, model, tokenizer):\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    outputs = model.generate(**inputs)\n",
    "    translated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return translated_texts\n",
    "\n",
    "def back_translate(sentence):\n",
    "    en = translate([sentence], model_zh2en, tok_zh2en)[0]\n",
    "    zh = translate([en], model_en2zh, tok_en2zh)[0]\n",
    "    return zh\n",
    "\n",
    "model_zh2en, tok_zh2en = load_model('zh', 'en')\n",
    "model_en2zh, tok_en2zh = load_model('en', 'zh')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b069f62a",
   "metadata": {},
   "source": [
    "我们从中性情感样本中抽取 $50\\%$ 的数据进行回译增强，生成新的中性样本。得到的增强样本数量为\n",
    "$$66472 \\times 50\\% = 33236$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b012a508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['你们俩 谈恋爱不准耽误学习！',\n",
       " '真假？不是说深圳是最后的行程吗',\n",
       " '黄礼志，像翻盖手机一样的女人。',\n",
       " '2025年',\n",
       " '一夜花开',\n",
       " 'hola！',\n",
       " '深呼吸深呼吸',\n",
       " '抖抖发了12条，其他的草稿发在微博不过分吧',\n",
       " '心理治疗的本质，不过是一种自律的工具。 斯科特派克少有人走的路',\n",
       " '往事有底片为证']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_to_translate = neutral_data[\"text\"].sample(frac=0.5, random_state=42).to_list()\n",
    "\n",
    "texts_to_translate[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724b3ce4",
   "metadata": {},
   "source": [
    "### 3.2.3 批量回译实现\n",
    "\n",
    "为了提高回译效率，我们使用 **批量回译** 的方法处理数据。\n",
    "\n",
    "回译过程中，每完成 100 条文本的处理后，就将结果存储一次，存储位置为 `data/preparation/texts_back_translated.csv`。\n",
    "\n",
    "实验表明，批量回译相较于逐条回译，效率可提升约 2 倍。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83198487",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def batch_back_translate(sentences, saved_path=\"data/preparation/texts_back_translated.csv\", \n",
    "                                   batch_size=32, max_length=512, save_every=100):\n",
    "    \"\"\"\n",
    "    增量保存版本的批量回译函数\n",
    "    \n",
    "    Args:\n",
    "        sentences: 要翻译的句子列表\n",
    "        saved_path: CSV保存路径\n",
    "        batch_size: 批处理大小\n",
    "        max_length: 最大序列长度\n",
    "        save_every: 每处理多少条数据保存一次\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame包含所有回译结果\n",
    "    \"\"\"\n",
    "    # 检查是否已有部分结果\n",
    "    if os.path.exists(saved_path):\n",
    "        existing_df = pd.read_csv(saved_path, encoding='utf-8-sig')\n",
    "        processed_count = len(existing_df)\n",
    "        print(f\"发现已处理 {processed_count} 条数据，从第 {processed_count + 1} 条开始...\")\n",
    "        sentences = sentences[processed_count:]\n",
    "        if not sentences:\n",
    "            print(\"所有数据已处理完成！\")\n",
    "            return existing_df\n",
    "    else:\n",
    "        existing_df = pd.DataFrame(columns=['original_text', 'augmented_text', 'intermediate_english'])\n",
    "        processed_count = 0\n",
    "    \n",
    "    # 创建保存目录\n",
    "    os.makedirs(os.path.dirname(saved_path), exist_ok=True)\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    print(f\"开始处理剩余 {len(sentences)} 条文本...\")\n",
    "    \n",
    "    # 分批处理\n",
    "    for i in tqdm(range(0, len(sentences), batch_size), desc=\"增量回译中\"):\n",
    "        batch = sentences[i:i + batch_size]\n",
    "        \n",
    "        try:\n",
    "            # 执行回译\n",
    "            inputs_zh = tok_zh2en(batch, return_tensors=\"pt\", padding=True, \n",
    "                                 truncation=True, max_length=max_length)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                en_outputs = model_zh2en.generate(**inputs_zh, max_length=max_length, \n",
    "                                                num_beams=4, early_stopping=True)\n",
    "                en_texts = tok_zh2en.batch_decode(en_outputs, skip_special_tokens=True)\n",
    "            \n",
    "            inputs_en = tok_en2zh(en_texts, return_tensors=\"pt\", padding=True, \n",
    "                                 truncation=True, max_length=max_length)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                zh_outputs = model_en2zh.generate(**inputs_en, max_length=max_length,\n",
    "                                                num_beams=4, early_stopping=True)\n",
    "                zh_texts = tok_en2zh.batch_decode(zh_outputs, skip_special_tokens=True)\n",
    "            \n",
    "            # 收集批次结果\n",
    "            batch_results = []\n",
    "            for j, (original, translated, english) in enumerate(zip(batch, zh_texts, en_texts)):\n",
    "                batch_results.append({\n",
    "                    'original_text': original,\n",
    "                    'augmented_text': translated,\n",
    "                    'intermediate_english': english\n",
    "                })\n",
    "            \n",
    "            all_results.extend(batch_results)\n",
    "            \n",
    "            # 定期保存结果\n",
    "            if len(all_results) >= save_every or i + batch_size >= len(sentences):\n",
    "                # 合并现有数据和新数据\n",
    "                new_df = pd.DataFrame(all_results)\n",
    "                combined_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "                \n",
    "                # 保存到文件\n",
    "                combined_df.to_csv(saved_path, index=False, encoding='utf-8-sig')\n",
    "                \n",
    "                print(f\"已保存 {len(combined_df)} 条数据到 {saved_path}\")\n",
    "                \n",
    "                # 更新现有数据框\n",
    "                existing_df = combined_df\n",
    "                all_results = []  # 清空临时结果\n",
    "            \n",
    "            # 清理GPU内存\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"处理批次时出错: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"所有数据处理完成！最终结果: {len(existing_df)} 条\")\n",
    "    return existing_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d3a3ec",
   "metadata": {},
   "source": [
    "### 3.2.4 回译结果处理\n",
    "\n",
    "经观察，回译生成的数据中存在极少量的缺失值，这里使用 `dropna()` 方法将其删除。\n",
    "\n",
    "另外，为了保持数据特征的统一，我们限定只取回译后文本长度介于 3 到 50 之间的样本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "011c5a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "发现已处理 33236 条数据，从第 33237 条开始...\n",
      "所有数据已处理完成！\n",
      "回译结果大小：33236\n",
      "删除缺失值后回译结果大小：33234\n",
      "符合文本长度要求的回译结果大小：32794\n"
     ]
    }
   ],
   "source": [
    "translated_results = batch_back_translate(texts_to_translate)\n",
    "print(f\"回译结果大小：{len(translated_results)}\")\n",
    "translated_results = translated_results.dropna()\n",
    "print(f\"删除缺失值后回译结果大小：{len(translated_results)}\")\n",
    "translated_results = translated_results[\n",
    "    translated_results['augmented_text'].str.len().between(3, 50)\n",
    "]\n",
    "print(f\"符合文本长度要求的回译结果大小：{len(translated_results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3579a3",
   "metadata": {},
   "source": [
    "### 3.2.5 回译结果展示\n",
    "\n",
    "我们从回译结果中随机抽取 5 条样本进行展示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46c6037b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "随机抽取回译样本：\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "original_text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "augmented_text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "intermediate_english",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "215115e8-3db6-4509-95d8-9f3b1eded570",
       "rows": [
        [
         "25002",
         "仿佛把晚饭和午休进化掉了",
         "这就像晚餐和午餐休息 已经进化。",
         "It's like dinner and lunch break have evolved."
        ],
        [
         "27328",
         "想起有个采访里很多gay对拉拉的刻板印象是是严肃无聊",
         "在采访中想起很多关于Lara的同性恋定型观念是很无聊的。",
         "It's a serious bore to remember a lot of gay stereotypes about Lara in an interview."
        ],
        [
         "3085",
         "吃撑了 到现在还在消化",
         "还在消化中",
         "It's still digesting."
        ],
        [
         "20864",
         "最近everyday都是排练日",
         "最近每天都是排练日",
         "Lately, every day is rehearsal day."
        ],
        [
         "32778",
         "T1 还行，输了但是很精彩，下次加油。",
         "T1没事,输了但聪明,来吧",
         "T1's okay. Lose but brilliant. Come on."
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "      <th>augmented_text</th>\n",
       "      <th>intermediate_english</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25002</th>\n",
       "      <td>仿佛把晚饭和午休进化掉了</td>\n",
       "      <td>这就像晚餐和午餐休息 已经进化。</td>\n",
       "      <td>It's like dinner and lunch break have evolved.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27328</th>\n",
       "      <td>想起有个采访里很多gay对拉拉的刻板印象是是严肃无聊</td>\n",
       "      <td>在采访中想起很多关于Lara的同性恋定型观念是很无聊的。</td>\n",
       "      <td>It's a serious bore to remember a lot of gay s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3085</th>\n",
       "      <td>吃撑了 到现在还在消化</td>\n",
       "      <td>还在消化中</td>\n",
       "      <td>It's still digesting.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20864</th>\n",
       "      <td>最近everyday都是排练日</td>\n",
       "      <td>最近每天都是排练日</td>\n",
       "      <td>Lately, every day is rehearsal day.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32778</th>\n",
       "      <td>T1 还行，输了但是很精彩，下次加油。</td>\n",
       "      <td>T1没事,输了但聪明,来吧</td>\n",
       "      <td>T1's okay. Lose but brilliant. Come on.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    original_text                augmented_text  \\\n",
       "25002                仿佛把晚饭和午休进化掉了              这就像晚餐和午餐休息 已经进化。   \n",
       "27328  想起有个采访里很多gay对拉拉的刻板印象是是严肃无聊  在采访中想起很多关于Lara的同性恋定型观念是很无聊的。   \n",
       "3085                  吃撑了 到现在还在消化                         还在消化中   \n",
       "20864             最近everyday都是排练日                     最近每天都是排练日   \n",
       "32778         T1 还行，输了但是很精彩，下次加油。                 T1没事,输了但聪明,来吧   \n",
       "\n",
       "                                    intermediate_english  \n",
       "25002     It's like dinner and lunch break have evolved.  \n",
       "27328  It's a serious bore to remember a lot of gay s...  \n",
       "3085                               It's still digesting.  \n",
       "20864                Lately, every day is rehearsal day.  \n",
       "32778            T1's okay. Lose but brilliant. Come on.  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"随机抽取回译样本：\")\n",
    "\n",
    "translated_results.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cab1d9",
   "metadata": {},
   "source": [
    "从抽取的回译结果中可以看到，微博作为网络社交平台，用户发布的内容往往较为口语化，往往并不严格按照书面语的规范组织文字，这导致了回译生成的文本可能出现语义偏差、结构混乱、信息缺失等问题。\n",
    "\n",
    "不过，尽管回译生成的文本在语义上存在细微偏差，但在情感倾向上与原文本基本保持一致，因此，这些回译生成的文本仍然可以作为有效的中性情感样本用于数据增强，帮助模型更好地学习中性情感的表达方式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c7546c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "回译前数据集中性样本数量: 66472\n",
      "回译后数据集中性样本数量: 99266\n"
     ]
    }
   ],
   "source": [
    "translated_data = DataFrame([\n",
    "    (text, \"中立\", 0) for text in translated_results[\"augmented_text\"]\n",
    "], columns=[\"text\", \"sentiment_type\", \"sentiment_polarity\"])\n",
    "\n",
    "# 将回译数据加入到原始 DataFrame 中\n",
    "df_aug = pd.concat([df, translated_data], ignore_index=True)\n",
    "    \n",
    "# 打乱\n",
    "df_aug = df_aug.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "print(f\"回译前数据集中性样本数量: {len(neutral_data)}\")\n",
    "print(f\"回译后数据集中性样本数量: {len(df_aug[df_aug['sentiment_type'] == '中立'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296b6532",
   "metadata": {},
   "source": [
    "### 3.2.6 增强结果分析\n",
    "\n",
    "最后，我们查看数据增强后数据集中标签的分布情况："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bfd1e0f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment_type\n",
      "积极    167778\n",
      "消极    124334\n",
      "中立     99266\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "type_counts = df_aug[\"sentiment_type\"].value_counts()\n",
    "print(type_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdaf6585",
   "metadata": {},
   "source": [
    "和 `02Feature Engineering and Data Analysis.ipynb` 一样，我们计算最大类/最小类 比率 IR 与变异系数 CV：\n",
    "> - $IR \\leq 1.5$ → 非常平衡\n",
    "> - $1.5 < IR \\leq 3$ → 轻度不平衡\n",
    "> - $IR > 3$ → 明显不平衡\n",
    "> - $IR > 10$ → 高度不平衡，需要采样或加权"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c68ab72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据增强前 IR = 2.52\n",
      "数据增强后 IR = 1.69\n"
     ]
    }
   ],
   "source": [
    "type_counts_before = df[\"sentiment_type\"].value_counts()\n",
    "\n",
    "ir_before = type_counts_before.max() / type_counts_before.min()\n",
    "ir = type_counts.max() / type_counts.min()\n",
    "\n",
    "print(f\"数据增强前 IR = {ir_before:.2f}\")\n",
    "print(f\"数据增强后 IR = {ir:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef58715e",
   "metadata": {},
   "source": [
    "从 IR 可以看出不平衡程度有所减小。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d82bb1",
   "metadata": {},
   "source": [
    "> - $CV \\leq 0.1$ → 非常平衡\n",
    "> - $0.1 < CV \\leq 0.3$ → 可接受\n",
    "> - $CV > 0.3$ → 明显不平衡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "866c7cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据增强前 CV = 0.43\n",
      "数据增强后 CV = 0.27\n"
     ]
    }
   ],
   "source": [
    "cv_before = type_counts_before.std() / type_counts_before.mean()\n",
    "cv = type_counts.std() / type_counts.mean()\n",
    "\n",
    "print(f\"数据增强前 CV = {cv_before:.2f}\")\n",
    "print(f\"数据增强后 CV = {cv:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3d66bc",
   "metadata": {},
   "source": [
    "从 CV 同样可以看到不平衡程度有所减弱。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a717a3f3",
   "metadata": {},
   "source": [
    "在这个 Notebook 中，我们为了缓解数据集的类别不平衡问题，采用了回译的方法对中性情感样本进行了数据增强。\n",
    "\n",
    "通过回译生成了大量新的中性样本，丰富了数据集的多样性，有助于提升模型在中性情感识别上的表现。\n",
    "\n",
    "然而，正如 IR 和 CV 值所示，数据集**仍然存在一定程度的不平衡问题**。\n",
    "\n",
    "在后续的模型训练过程中，我们将考虑结合其他技术手段，如**加权损失函数**，进一步缓解类别不平衡对模型性能的影响。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c611b2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'data/preparation/weibo_augmented.csv'\n",
    "\n",
    "df_aug.to_csv(save_path, index=False, encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpllma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
