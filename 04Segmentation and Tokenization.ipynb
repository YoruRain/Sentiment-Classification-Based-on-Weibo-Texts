{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5a4fcf5",
   "metadata": {},
   "source": [
    "# 04 中文分词与词元化\n",
    "\n",
    "本 Notebook 中，我们将对文本数据进行中文分词与词元化处理，为后续的模型训练做好准备。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed05ded3",
   "metadata": {},
   "source": [
    "首先，我们读取从 `03 Data Augmentation.ipynb` 中保存的增强数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5c9050d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据: 391378 行\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "sentiment_polarity",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "7d761b88-9123-47b7-9df4-1367750f49aa",
       "rows": [
        [
         "0",
         "服了 怎么见别人的哥哥就是个小怂货 见自己哥哥也是小怂货！！ 我下次一定要上去玩弄他们！",
         "-1"
        ],
        [
         "1",
         "fo的男娘又发了一大堆美丽照片 我挨个连赞的样子真的很丑陋",
         "-1"
        ],
        [
         "2",
         "迟到的郁金香还有腿",
         "0"
        ],
        [
         "3",
         "好恶毒的商战",
         "-1"
        ],
        [
         "4",
         "十一就是没出门前就想出去玩，出来后就后悔",
         "-1"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment_polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>服了 怎么见别人的哥哥就是个小怂货 见自己哥哥也是小怂货！！ 我下次一定要上去玩弄他们！</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fo的男娘又发了一大堆美丽照片 我挨个连赞的样子真的很丑陋</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>迟到的郁金香还有腿</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>好恶毒的商战</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>十一就是没出门前就想出去玩，出来后就后悔</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           text  sentiment_polarity\n",
       "0  服了 怎么见别人的哥哥就是个小怂货 见自己哥哥也是小怂货！！ 我下次一定要上去玩弄他们！                  -1\n",
       "1                 fo的男娘又发了一大堆美丽照片 我挨个连赞的样子真的很丑陋                  -1\n",
       "2                                     迟到的郁金香还有腿                   0\n",
       "3                                        好恶毒的商战                  -1\n",
       "4                          十一就是没出门前就想出去玩，出来后就后悔                  -1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "read_path = 'data/preparation/weibo_augmented.csv'\n",
    "\n",
    "# 读取数据\n",
    "df = pd.read_csv(read_path, encoding='utf-8-sig')\n",
    "\n",
    "df = df[[\"text\", \"sentiment_polarity\"]]\n",
    "print(f\"数据: {len(df)} 行\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91d6c76",
   "metadata": {},
   "source": [
    "## 4.1 标签映射"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a21b28e",
   "metadata": {},
   "source": [
    "可以看到，数据集使用 `[-1, 0, 1]` 表示情感极性。\n",
    "\n",
    "后续训练模型时会发现，反向传播时使用的损失函数（`CrossEntropyLoss`）要求样本的标签为 `0 ~ num_classes-1` 之间的整数，因此需要将情感标签从原始的 `[-1, 0, 1]` 映射到 `[0, 1, 2]`。\n",
    "\n",
    "这里使用 `remap_labels` 函数来实现标签的映射。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "72e3b49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "2    167778\n",
      "0    124334\n",
      "1     99266\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pandas import DataFrame\n",
    "\n",
    "def remap_labels(data: DataFrame, label_mapping={-1:0, 0:1, 1:2}):\n",
    "    if 2 in data[\"label\"].values:\n",
    "        return data\n",
    "    remapped_data = []\n",
    "    for tokens, old_label in data.values:\n",
    "        new_label = label_mapping.get(old_label, old_label)\n",
    "        remapped_data.append((tokens, new_label))\n",
    "    return DataFrame(remapped_data, columns=[\"text\", \"label\"])\n",
    "\n",
    "df = df.rename(columns={\"sentiment_polarity\": \"label\"})\n",
    "df = remap_labels(df)\n",
    "print(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f968908",
   "metadata": {},
   "source": [
    "## 4.2 划分数据集\n",
    "\n",
    "在机器学习任务中，通常需要将数据集划分为**训练集**（Train）、**验证集**（Validation）和 **测试集**（Test），以便模型能够在不同的数据上进行训练和评估。\n",
    "\n",
    "训练集用于模型的训练，验证集用于调参和选择最佳模型，测试集用于最终评估模型的性能。\n",
    "\n",
    "`sklearn.model_selection` 模块提供了函数 `train_test_split`，可以轻松地将数据集划分为不同的子集。\n",
    "\n",
    "这里，我们采用机器学习任务中常用的 **70% 训练集 + 15% 验证集 + 15% 测试集** 的划分比例。\n",
    "\n",
    "> 机器学习领域，还有一种构建验证集的方法，称为 **交叉验证**（Cross-Validation）。不过，鉴于本项目的数据集已经相当庞大（总共约40万条样本数据），单次测试集上的性能估计已经非常接近模型真实的泛化性能，因此我们选择了更为简单直接的单次划分方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bb446988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(273964, 58707, 58707)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df[\"text\"]\n",
    "y = df[\"label\"]\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "train_raw = DataFrame({\"text\": X_train, \"label\": y_train})\n",
    "val_raw = DataFrame({\"text\": X_val, \"label\": y_val})\n",
    "test_raw = DataFrame({\"text\": X_test, \"label\": y_test})\n",
    "\n",
    "len(train_raw), len(val_raw), len(test_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4aa3f11",
   "metadata": {},
   "source": [
    "接下来，我们要进行文本数据的分词与词元化处理。\n",
    "\n",
    "**分词**（Word Segmentation）是中文自然语言处理中的基础任务，旨在将连续的汉字序列切分成有意义的词语单元。\n",
    "\n",
    "与英文等语言不同，中文文本中词语之间 **没有明确的空格分隔符**，这使得分词成为中文 NLP 任务中的一个关键步骤。\n",
    "\n",
    "**词元化**（Tokenization）是将文本转换为模型可处理的数值形式的过程。\n",
    "\n",
    "对于基于 **词向量**（Word Embeddings）的模型，词元化通常涉及将分词后的词语映射到唯一的整数索引，形成**词汇表**（Vocabulary）。\n",
    "\n",
    "接下来，我们将逐步完成分词、构建词汇表以及词元化的过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0479a8",
   "metadata": {},
   "source": [
    "## 4.3 分词\n",
    "\n",
    "我们使用 LTP 作为分词工具。\n",
    "\n",
    "LTP（Language Technology Platform，语言技术平台）是哈尔滨工业大学社会计算与信息检索研究中心（HIT-SCIR）历时多年研发的一整套高效、高精度的中文自然语言处理开源基础技术平台。\n",
    "\n",
    "相较于常用的分词工具如 Jieba，LTP 在分词方面各方面的表现均较为优秀，适合用于对文本进行高质量的分词处理。\n",
    "\n",
    "与回译类似，这里我们也采用批处理的方式对文本进行分词，以提升处理效率。\n",
    "\n",
    "我们将依次对训练集、验证集和测试集的文本数据进行分词处理，处理好的数据存储于 `data/preparation/*_segmented.txt` 中，其中 `*` 分别对应 `train`、`val` 和 `test`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "689a9501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始处理训练集...\n",
      "文件 data/preparation/train_segmented.txt 已存在，直接读取。\n",
      "\n",
      "开始处理验证集...\n",
      "文件 data/preparation/val_segmented.txt 已存在，直接读取。\n",
      "\n",
      "开始处理测试集...\n",
      "文件 data/preparation/test_segmented.txt 已存在，直接读取。\n"
     ]
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "\n",
    "from ltp import LTP\n",
    "def segment_data(X, y, ltp, file_path='', batch_size=500):  #@save\n",
    "    \"\"\"\n",
    "    分批处理数据以避免内存不足问题\n",
    "    batch_size: 每批处理的数据量，默认1000条\n",
    "    \"\"\"\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"文件 {file_path} 已存在，直接读取。\")\n",
    "        return load_data(file_path, sep='<sp>', is_segmented=True)\n",
    "    if not isinstance(X, list):\n",
    "        X = X.tolist()\n",
    "    if not isinstance(y, list):\n",
    "        y = y.tolist()\n",
    "    \n",
    "\n",
    "    segmented_data = []\n",
    "    \n",
    "    print(f\"开始处理 {len(X)} 条数据，批大小: {batch_size}\")\n",
    "    \n",
    "    if len(file_path) > 0:\n",
    "        # 打开文件准备写入\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            # 分批处理\n",
    "            for i in range(0, len(X), batch_size):\n",
    "                batch_end = min(i + batch_size, len(X))\n",
    "                batch_X = X[i:batch_end]\n",
    "                batch_y = y[i:batch_end]\n",
    "                \n",
    "                print(f\"正在处理第 {i//batch_size + 1} 批，数据范围: {i}-{batch_end-1}\")\n",
    "                \n",
    "                # 对当前批次进行分词\n",
    "                segment = ltp.pipeline(batch_X, tasks=['cws'], return_dict=False)[0]\n",
    "                \n",
    "                # 写入文件\n",
    "                for sublist, label in zip(segment, batch_y):\n",
    "                    segmented_data.append((sublist, label))\n",
    "                    f.write('<sp>'.join(sublist) + ':' + str(label) + '\\n')\n",
    "                \n",
    "                print(f\"第 {i//batch_size + 1} 批处理完成\")\n",
    "    \n",
    "    print(\"所有数据处理完成！\")\n",
    "    return DataFrame(segmented_data, columns=[\"text\", \"label\"])\n",
    "\n",
    "def load_data(  #@save\n",
    "        file_path: str, \n",
    "        sep='', \n",
    "        is_segmented=False, \n",
    "        is_indexed=False) -> DataFrame:\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8-sig') as f:\n",
    "        for line in f:\n",
    "            X, y = line.rsplit(':', 1)\n",
    "            if is_segmented:\n",
    "                X = [token.strip() for token in X.split(sep) if token.strip() != '']\n",
    "            elif is_indexed:\n",
    "                X = list(map(int, X.split(sep)))\n",
    "            y = int(float(y))\n",
    "            data.append((X, y))\n",
    "    return DataFrame(data, columns=[\"text\", \"label\"])\n",
    "\n",
    "\n",
    "# 初始化 LTP 分词器\n",
    "ltp = LTP()\n",
    "\n",
    "# 分批处理数据，使用较小的批大小以避免内存问题\n",
    "print(\"开始处理训练集...\")\n",
    "train_seg = segment_data(X_train, y_train, ltp, 'data/preparation/train_segmented.txt', batch_size=500)\n",
    "\n",
    "print(\"\\n开始处理验证集...\")\n",
    "val_seg = segment_data(X_val, y_val, ltp, 'data/preparation/val_segmented.txt', batch_size=500)\n",
    "\n",
    "print(\"\\n开始处理测试集...\")\n",
    "test_seg = segment_data(X_test, y_test, ltp, 'data/preparation/test_segmented.txt', batch_size=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339e9d93",
   "metadata": {},
   "source": [
    "> 上面的代码中，`load_data` 的函数签名后有 `#@save` 标志。标志说明该函数是一个复用性较高的工具函数，我们会在后续的 Notebook 中多次使用它。因此，我们将其保存到 `MyModule` 工具包中。在今后的 Notebook 中，将直接从 `MyModule` 中导入该函数直接使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcd457c",
   "metadata": {},
   "source": [
    "## 4.4 构建词汇表\n",
    "\n",
    "无论是使用深度学习还是使用传统的统计机器学习方法处理自然语言，都需要将输入的语言符号（通常为词元）映射为大于等于 0、小于词表大小的整数，该整数也被称为词元的索引值或下标。\n",
    "\n",
    "例如：\n",
    "- **输入文本**：`今天天气很好，我很开心。`\n",
    "- **分词结果**：`['今天', '天气', '很', '好', '，', '我', '很', '开心', '。']`\n",
    "- **序列化表示**：`[12, 45, 7, 23, 3, 9, 7, 56, 4]`\n",
    "\n",
    "我们将实现这种词元与索引之间映射的数据结构称为 **词汇表**（Vocabulary）。\n",
    "\n",
    "下面，我们实现一个 `Vocab` 类，实现词元和索引之间的相互映射。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4159cbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class Vocab:  #@save\n",
    "    def __init__(self, tokens=None) -> None:\n",
    "        self.idx_to_token = list()\n",
    "        self.token_to_idx = dict()\n",
    "\n",
    "        if tokens is not None:\n",
    "            if \"<unk>\" not in tokens:\n",
    "                tokens = tokens + [\"<unk\"]\n",
    "            for token in tokens:\n",
    "                self.idx_to_token.append(token)\n",
    "                self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
    "            self.unk = self.token_to_idx[\"<unk>\"]\n",
    "\n",
    "    @classmethod\n",
    "    def build(cls, text, min_freq=1, reserved_tokens=None):\n",
    "        token_freqs = defaultdict(int)\n",
    "        for sentence in text:\n",
    "            for token in sentence:\n",
    "                token_freqs[token] += 1\n",
    "        \n",
    "        uniq_tokens = [\"<unk>\"] + (reserved_tokens if reserved_tokens else [])\n",
    "        uniq_tokens += [token for token, freq in token_freqs.items() \n",
    "                        if freq >= min_freq and token != \"<unk>\"]\n",
    "        return cls(uniq_tokens)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.idx_to_token)\n",
    "    \n",
    "    def __getitem__(self, token):\n",
    "        \"\"\"查找输入词元对应的索引值，若不存在，则返回<unk>的索引值（0）\"\"\"\n",
    "        return self.token_to_idx.get(token, self.unk)\n",
    "    \n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        \"\"\"查找一系列输入词元的索引值\"\"\"\n",
    "        return [self[token] for token in tokens]\n",
    "    \n",
    "    def convert_ids_to_tokens(self, indices):\n",
    "        \"\"\"查找一系列输入索引值对应的词元\"\"\"\n",
    "        return [self.idx_to_token[index] for index in indices]\n",
    "\n",
    "def save_vocab(vocab: Vocab, file_path: str):  #@save\n",
    "    with open(file_path, 'w', encoding='utf-8-sig') as f:\n",
    "        f.write('\\n'.join(vocab.idx_to_token))\n",
    "\n",
    "def read_vocab(file_path: str) -> Vocab:  #@save\n",
    "    with open(file_path, 'r', encoding='utf-8-sig') as f:\n",
    "        tokens = f.read().split('\\n')\n",
    "    return Vocab(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abb1e16",
   "metadata": {},
   "source": [
    "### 4.4.1 问题提出\n",
    "\n",
    "在实例化 `Vocab` 类创建词汇表对象前，我们需要解决两个重要的问题：\n",
    "\n",
    "1. 应该在什么数据集上构建词汇表？\n",
    "2. 如何设定类方法 `build` 中的最小词频参数 `min_freq`？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add34dad",
   "metadata": {},
   "source": [
    "### 4.4.2 构建词汇表的数据集选择\n",
    "\n",
    "首先来看第1个问题。\n",
    "\n",
    "问题具体来说，我们是应该使用 **训练集、验证集和测试集组成的整个大数据集上** 构建词汇表，还是 **仅使用训练集** 来构建词汇表？\n",
    "\n",
    "在这里，我们采用仅使用训练集来构建词汇表的策略。原因有二：\n",
    "- **防止数据泄漏（Data Leakage）**：使用验证集和测试集的数据来构建词汇表可能会导致模型在训练过程中“看到”这些数据，从而影响模型的泛化能力。为了确保模型的评估结果真实反映其在未见数据上的表现，应该避免在训练过程中使用验证集和测试集的信息\n",
    "- **模拟真实应用场景**：在实际应用中，模型通常只能访问训练数据，而无法预先了解未来的验证或测试数据。因此，仅使用训练集构建词汇表更符合实际应用场景，有助于提高模型在真实环境中的表现\n",
    "\n",
    "> 尽管也有一些研究称，同时使用验证集和测试集构建词表并不会导致数据泄露（因为词表作为统计资源，并不包含标签信息或特征分布信息），但为了尽可能模拟现实的应用场景，以及作为工业应用上的常用策略，我们仍然仅使用训练集来构建词汇表。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df18d0b4",
   "metadata": {},
   "source": [
    "### 4.4.3 最小词频参数设定\n",
    "\n",
    "接着是第2个问题：如何设定 **最小词频参数** `min_freq`？\n",
    "\n",
    "最小词频参数 `min_freq` 是构建词表时用来过滤低频词的重要参数，它将直接影响词表大小、模型的泛化能力和训练速度等。\n",
    "\n",
    "具体而言，`min_freq` 的作用为：\n",
    "- 对数据集中出现的所有词元进行计数统计，只有当某个词出现次数大于等于 `min_freq` 时，才会被保留到词表中\n",
    "- 否则将视为“低频词”，使用 `<unk>`（未知词词元符号）进行替代\n",
    "\n",
    "如果 `min_freq` 设置太小，优点在于能够最大程度保留词汇信息，缺点在于会导致词表过大，进而导致模型训练较慢，并且带来较大噪声；\n",
    "\n",
    "相反，如果 `min_freq` 设置过大，优点在于能够显著减少词表大小，提高训练速度，缺点在于会丢失太多低频但重要的词，影响情感分类准确率。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc3e07f",
   "metadata": {},
   "source": [
    "关于 `min_freq` 的设置，可以参考下面的经验标准：\n",
    "|数据量|推荐 `min_freq`|\n",
    "|-|-|\n",
    "|<1 万条文本|1|\n",
    "|1~10 万条文本|2~3|\n",
    "|10~100 万条文本|3~5|\n",
    "|>100万条文本|5~10|\n",
    "\n",
    "在前面的部分，我们已知训练集的大小约为 27 万条文本，因此推荐 `min_freq` 设置在 3~5 之间。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316a8a22",
   "metadata": {},
   "source": [
    "#### (1) 词频分布统计与分析\n",
    "\n",
    "进一步地，我们可以通过观察词频分布与统计覆盖率的方法，更加科学与定量地确定 `min_freq` 的值。\n",
    "\n",
    "首先，我们统计训练集中各词的词频分布情况，并绘制词频分布图。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e3d82aea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "token_counts = Counter(token for text in train_seg[\"text\"] for token in text)\n",
    "freqs = list(token_counts.values())\n",
    "plt.hist(freqs, bins=100, range=(0,50))\n",
    "plt.xlabel('Token Count')\n",
    "plt.ylabel('Token Frequency')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7039b6c",
   "metadata": {},
   "source": [
    "通过观察图中词频与词汇数量的关系，我们可以发现大部分词的词频较低，而少数高频词占据了大部分文本内容。这符合中文微博语料的长尾分布特性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9b64a6",
   "metadata": {},
   "source": [
    "我们可以查看频率最高的前50个词元："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fc8cefd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('的', 144643),\n",
       " ('了', 106196),\n",
       " ('，', 94639),\n",
       " ('我', 91131),\n",
       " ('是', 47209),\n",
       " ('。', 44763),\n",
       " ('不', 44749),\n",
       " ('！', 40062),\n",
       " ('一', 37043),\n",
       " ('好', 32325),\n",
       " ('啊', 31775),\n",
       " ('就', 23228),\n",
       " ('在', 23185),\n",
       " ('都', 22193),\n",
       " ('想', 19726),\n",
       " ('你', 19102),\n",
       " ('有', 17632),\n",
       " ('要', 16596),\n",
       " ('吃', 16288),\n",
       " ('人', 16187),\n",
       " ('天', 15801),\n",
       " ('到', 15538),\n",
       " ('这', 14785),\n",
       " ('个', 14601),\n",
       " ('也', 14165),\n",
       " ('看', 13759),\n",
       " ('又', 13629),\n",
       " ('能', 13525),\n",
       " ('很', 13305),\n",
       " ('和', 12475),\n",
       " ('没', 12334),\n",
       " ('会', 11944),\n",
       " ('今天', 11904),\n",
       " ('一个', 11185),\n",
       " ('去', 11088),\n",
       " ('真', 11060),\n",
       " ('小', 11058),\n",
       " ('上', 10752),\n",
       " ('还', 10637),\n",
       " ('太', 10575),\n",
       " ('自己', 10520),\n",
       " ('来', 9923),\n",
       " (',', 9155),\n",
       " ('着', 8818),\n",
       " ('给', 8203),\n",
       " ('多', 8061),\n",
       " ('说', 7670),\n",
       " ('得', 7591),\n",
       " ('这个', 7464),\n",
       " ('？', 7384)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_counts.most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9321b2b9",
   "metadata": {},
   "source": [
    "对高频词稍作分析：\n",
    "- 出现  **标点符号**：如 `，`、`。`、`！` 等\n",
    "    - 对于分句作用的符号，如 `，`、`。`、`、` 等，对于情感表达的作用不大，可以考虑去除。\n",
    "    - 对于表示情感的符号，如 `！`、`？` 等，可以考虑保留。\n",
    "- 出现 **虚词**：如 `的`、`了`、`在`\n",
    "    - 尽管此类词语在语义上作用不大，但：\n",
    "        - 对传统机器学习来说，可通过权重自动弱化\n",
    "        - 对深度模型来说，它们的词嵌入会自动学到较弱权重\n",
    "    - 因此考虑保留此类词语\n",
    "- 一些 **附带空格的词元也被划分为独立的词元**，例如 `了` 和 `了 `，二者本质上属于同一个词元 \n",
    "    - 这类词元属于分词阶段的瑕疵，需要进行修正，以避免同一词语被划分为多个不同的词元。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b66a7db",
   "metadata": {},
   "source": [
    "接着，查看词元计数器中出现的所有标点符号："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "269e4c15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('?', 2101),\n",
       " ('，', 94639),\n",
       " ('：', 5259),\n",
       " ('。', 44763),\n",
       " ('（', 4452),\n",
       " ('）', 3271),\n",
       " (',', 9155),\n",
       " ('！', 40062),\n",
       " ('(', 2894),\n",
       " (')', 2856),\n",
       " ('？', 7384),\n",
       " ('【', 495),\n",
       " ('】', 474),\n",
       " (':', 459),\n",
       " ('；', 174),\n",
       " ('!', 384),\n",
       " ('_', 304),\n",
       " ('\"', 442),\n",
       " ('____', 4),\n",
       " ('《', 39),\n",
       " ('》', 39),\n",
       " ('、', 346),\n",
       " ('...', 36),\n",
       " ('“', 30),\n",
       " ('”', 26),\n",
       " ('[', 6),\n",
       " (']', 6),\n",
       " ('_________', 1),\n",
       " ('.', 55),\n",
       " ('*', 3),\n",
       " ('-', 40),\n",
       " ('#', 25),\n",
       " (';', 14),\n",
       " ('¶', 3),\n",
       " ('______', 2),\n",
       " ('__', 6),\n",
       " ('..', 1),\n",
       " ('%', 2),\n",
       " ('_____', 3),\n",
       " ('\\\\', 1),\n",
       " ('________', 1),\n",
       " (\"'\", 1),\n",
       " ('___________', 1),\n",
       " ('___', 6),\n",
       " ('」', 1),\n",
       " ('____________', 1),\n",
       " ('_______', 1),\n",
       " ('__________', 1)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unicodedata\n",
    "\n",
    "punct_tokens = [\n",
    "    (t, freq) for t, freq in token_counts.items() \n",
    "    if all(unicodedata.category(ch).startswith('P') \n",
    "           for ch in t)]\n",
    "\n",
    "punct_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d71dcf1",
   "metadata": {},
   "source": [
    "对标点符号的进一步分类与分析：\n",
    "- **表示情感的符号**：如 `！`、`？` 等，保留\n",
    "\n",
    "- **分句符号**：如 `，`、`。`、`；` 等，去除\n",
    "\n",
    "- **结构性符号**：如 `（`、`）`、`【`、`】` 等，去除\n",
    "\n",
    "- **异常符号（人工添加线）**：如 `_`、`__` 等，去除\n",
    "\n",
    "因此，需要对一些标点符号进行过滤，去除不必要的标点符号。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776bc27e",
   "metadata": {},
   "source": [
    "#### (2) 清洗分词数据集\n",
    "\n",
    "在清洗分词数据集的同时，也需要**将未分词的原始数据集中对应的数据一并清洗**，以保证分词数据集与原始数据集的一一对应关系。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e98a7c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始数据长度:\n",
      "train: seg=273964, raw=273964\n",
      "val: seg=58707, raw=58707\n",
      "test: seg=58707, raw=58707\n",
      "\n",
      "开始同步清理...\n",
      "开始清理数据，原始长度: seg=273964, raw=273964\n",
      "清理完成，删除了 1096 个样本\n",
      "清理后长度: seg=272868, raw=272868\n",
      "开始清理数据，原始长度: seg=58707, raw=58707\n",
      "清理完成，删除了 235 个样本\n",
      "清理后长度: seg=58472, raw=58472\n",
      "开始清理数据，原始长度: seg=58707, raw=58707\n",
      "清理完成，删除了 257 个样本\n",
      "清理后长度: seg=58450, raw=58450\n",
      "\n",
      "最终数据长度:\n",
      "train: seg=272868, raw=272868\n",
      "val: seg=58472, raw=58472\n",
      "test: seg=58450, raw=58450\n"
     ]
    }
   ],
   "source": [
    "def clean_punct_synchronized(seg_data: DataFrame, raw_data: DataFrame):\n",
    "    \"\"\"同步清理分词数据和原始数据，确保删除的是相同的样本\"\"\"\n",
    "    \n",
    "    remove_punct = {'，', ',', '。', '；', ';',  '：', ':',\n",
    "                     '\"', '\"', '\"', '（', '）', '(', ')',  \n",
    "                     '【', '】', '[', ']',  '《', '》', \n",
    "                     '#', '_'}\n",
    "    \n",
    "    cleaned_seg_data = []\n",
    "    cleaned_raw_data = []\n",
    "    removed_indices = []\n",
    "    \n",
    "    print(f\"开始清理数据，原始长度: seg={len(seg_data)}, raw={len(raw_data)}\")\n",
    "    \n",
    "    for i in range(len(seg_data)):\n",
    "        seg_tokens = seg_data.iloc[i]['text']\n",
    "        seg_label = seg_data.iloc[i]['label']\n",
    "        raw_text = raw_data.iloc[i]['text'] \n",
    "        raw_label = raw_data.iloc[i]['label']\n",
    "        \n",
    "        # 清理分词数据中的标点符号\n",
    "        # cleaned_tokens = [token.strip() for token in seg_tokens if token not in remove_punct]\n",
    "        cleaned_tokens = []\n",
    "        for token in seg_tokens:\n",
    "            if token not in remove_punct:\n",
    "                cleaned_tokens.extend(token.strip().split())\n",
    "        \n",
    "        # 检查清理后的文本是否过短\n",
    "        if len(''.join(cleaned_tokens)) <= 3:\n",
    "            removed_indices.append(i)\n",
    "        else:\n",
    "            cleaned_seg_data.append([cleaned_tokens, seg_label])\n",
    "            cleaned_raw_data.append([raw_text, raw_label])\n",
    "    \n",
    "    cleaned_seg_df = pd.DataFrame(cleaned_seg_data, columns=[\"text\", \"label\"])\n",
    "    cleaned_raw_df = pd.DataFrame(cleaned_raw_data, columns=[\"text\", \"label\"])\n",
    "    \n",
    "    print(f\"清理完成，删除了 {len(removed_indices)} 个样本\")\n",
    "    print(f\"清理后长度: seg={len(cleaned_seg_df)}, raw={len(cleaned_raw_df)}\")\n",
    "    \n",
    "    return cleaned_seg_df, cleaned_raw_df, removed_indices\n",
    "\n",
    "\n",
    "print(\"原始数据长度:\")\n",
    "print(f\"train: seg={len(train_seg)}, raw={len(train_raw)}\")\n",
    "print(f\"val: seg={len(val_seg)}, raw={len(val_raw)}\")\n",
    "print(f\"test: seg={len(test_seg)}, raw={len(test_raw)}\")\n",
    "\n",
    "# 使用正确的方法同步清理数据\n",
    "print(\"\\n开始同步清理...\")\n",
    "train_seg_cleaned, train_raw_cleaned, train_removed_indices = clean_punct_synchronized(train_seg, train_raw)\n",
    "val_seg_cleaned, val_raw_cleaned, val_removed_indices = clean_punct_synchronized(val_seg, val_raw)\n",
    "test_seg_cleaned, test_raw_cleaned, test_removed_indices = clean_punct_synchronized(test_seg, test_raw)\n",
    "\n",
    "\n",
    "print(f\"\\n最终数据长度:\")\n",
    "print(f\"train: seg={len(train_seg_cleaned)}, raw={len(train_raw_cleaned)}\")\n",
    "print(f\"val: seg={len(val_seg_cleaned)}, raw={len(val_raw_cleaned)}\")\n",
    "print(f\"test: seg={len(test_seg_cleaned)}, raw={len(test_raw_cleaned)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1678030a",
   "metadata": {},
   "source": [
    "再次查看训练集中出现频率最高的前 50 个词元："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fb45b666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('的', 144631),\n",
       " ('了', 106145),\n",
       " ('我', 91073),\n",
       " ('是', 47194),\n",
       " ('不', 44747),\n",
       " ('！', 40057),\n",
       " ('一', 37047),\n",
       " ('好', 32378),\n",
       " ('啊', 31760),\n",
       " ('就', 23226),\n",
       " ('在', 23175),\n",
       " ('都', 22196),\n",
       " ('想', 19734),\n",
       " ('你', 19084),\n",
       " ('有', 17630),\n",
       " ('要', 16597),\n",
       " ('吃', 16294),\n",
       " ('人', 16282),\n",
       " ('天', 15838),\n",
       " ('到', 15539),\n",
       " ('这', 14773),\n",
       " ('个', 14599),\n",
       " ('也', 14166),\n",
       " ('看', 13803),\n",
       " ('又', 13614),\n",
       " ('能', 13527),\n",
       " ('很', 13293),\n",
       " ('和', 12481),\n",
       " ('没', 12336),\n",
       " ('会', 11961),\n",
       " ('今天', 11901),\n",
       " ('一个', 11182),\n",
       " ('真', 11103),\n",
       " ('去', 11088),\n",
       " ('小', 11064),\n",
       " ('上', 10767),\n",
       " ('还', 10639),\n",
       " ('太', 10590),\n",
       " ('自己', 10520),\n",
       " ('来', 9924),\n",
       " ('着', 8811),\n",
       " ('给', 8203),\n",
       " ('多', 8062),\n",
       " ('说', 7675),\n",
       " ('得', 7594),\n",
       " ('这个', 7464),\n",
       " ('？', 7380),\n",
       " ('吧', 7329),\n",
       " ('爱', 7210),\n",
       " ('两', 7174)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_counts = Counter(token for text in train_seg_cleaned[\"text\"] for token in text)\n",
    "\n",
    "token_counts.most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc667e54",
   "metadata": {},
   "source": [
    "#### (3) 覆盖率统计\n",
    "\n",
    "清除掉一些低质量数据后，我们开始正式对 `min_freq` 的设置进行分析，使用的方法为：统计 **覆盖率**（Coverage）。\n",
    "\n",
    "覆盖率是指词表中包含的词元所覆盖的文本总词数占文本总词数的比例。\n",
    "\n",
    "通过计算不同 `min_freq` 值下的覆盖率，我们可以评估不同设置对文本表示能力的影响。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "530059c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_freq: 1\tcoverage: 1.0\n",
      "min_freq: 2\tcoverage: 0.9772082036310533\n",
      "min_freq: 3\tcoverage: 0.9680388332274191\n",
      "min_freq: 5\tcoverage: 0.9561645358083845\n",
      "min_freq: 10\tcoverage: 0.9370043515393062\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "token_counts = Counter(token for sentence in train_seg[\"text\"] for token in sentence)\n",
    "total_tokens = sum(token_counts.values())\n",
    "\n",
    "for mf in [1, 2, 3, 5, 10]:\n",
    "    kept_tokens = sum(count for count in token_counts.values() if count >= mf)\n",
    "    print(f\"min_freq: {mf}\\tcoverage: {kept_tokens / total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73ad6a6",
   "metadata": {},
   "source": [
    "一般来说，当覆盖率 `coverage` 不小于 95% 时，过滤效果比较合理。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bce22e",
   "metadata": {},
   "source": [
    "#### (4) 最终参数设定\n",
    "\n",
    "综上所述，构建词汇表时，我们设置最小词频 `min_freq=3`，即词频小于 3 的词元将被视为 `<UNK>` 词元，最终可达到约 96.8% 的覆盖率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d43d6e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab.build(train_seg_cleaned[\"text\"], min_freq=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a930ba4",
   "metadata": {},
   "source": [
    "查看词汇表大小："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7adf0484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35578"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5f32f7",
   "metadata": {},
   "source": [
    "可以看到，最终构建的词汇表大小为 35,578 个词元。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d933bdb3",
   "metadata": {},
   "source": [
    "## 4.5 索引化\n",
    "\n",
    "构建好词汇表后，就可以进行索引化处理。\n",
    "\n",
    "具体来说，对数据集中每个句子使用 `vocab.convert_tokens_to_ids` 方法，将句子中的词元序列转换为词元在词汇表中的索引序列，供后续模型训练使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4df8da36",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = DataFrame([\n",
    "    [vocab.convert_tokens_to_ids(text), label]\n",
    "    for text, label in train_seg_cleaned.values\n",
    "], columns=[\"text\", \"label\"])\n",
    "\n",
    "val_idx = DataFrame([\n",
    "    [vocab.convert_tokens_to_ids(text), label]\n",
    "    for text, label in val_seg_cleaned.values\n",
    "], columns=[\"text\", \"label\"])\n",
    "\n",
    "test_idx = DataFrame([\n",
    "    [vocab.convert_tokens_to_ids(text), label]\n",
    "    for text, label in test_seg_cleaned.values\n",
    "], columns=[\"text\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4251d04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样本 151019：\n",
      "原始句子： ['开会', '在', '干', '嘛']\n",
      "索引序列： [5582, 64, 119, 66]\n",
      "\n",
      "样本 181954：\n",
      "原始句子： ['麦蔻', '的', '呼噜声', '大', '到', '我', '在', '客厅', '琢磨', '这', '是', '什么', '动静']\n",
      "索引序列： [0, 5, 1888, 206, 244, 36, 64, 5482, 18180, 635, 89, 320, 2822]\n",
      "\n",
      "样本 59784：\n",
      "原始句子： ['起初', '是', '发现', '今天', '的', '图', '一', '好奇', '查', '了', '一下', '图', '二', '即使', '脱', '欧', '五', '年', '一直', '都', '是', '难兄难弟']\n",
      "索引序列： [19862, 89, 323, 61, 5, 3436, 177, 3186, 5800, 78, 149, 3436, 828, 5893, 2528, 6200, 452, 893, 592, 76, 89, 0]\n",
      "\n",
      "样本 119647：\n",
      "原始句子： ['今天', '的', 'Live', '演出', '是', '杰伦', '专场', '还有', '一', '首', '大同', '的', '特别', '的', '人']\n",
      "索引序列： [61, 5, 14158, 4738, 89, 20863, 7456, 20, 177, 1318, 7839, 5, 603, 5, 43]\n",
      "\n",
      "样本 60214：\n",
      "原始句子： ['哦买噶', '好萌', '的', '一', '对', '姐弟']\n",
      "索引序列： [9637, 15431, 5, 177, 375, 11814]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "for i in random.sample(range(len(train_seg)), 5):\n",
    "    print(f\"样本 {i+1}：\")\n",
    "    print(\"原始句子：\", train_seg_cleaned['text'].iloc[i])\n",
    "    print(\"索引序列：\", train_idx['text'].iloc[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d68eb4",
   "metadata": {},
   "source": [
    "## 4.6 数据保存\n",
    "\n",
    "最后，将数据集与词汇表保存至本地。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93be4b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已保存到 data/final/train_raw.txt。\n",
      "已保存到 data/final/val_raw.txt。\n",
      "已保存到 data/final/test_raw.txt。\n",
      "已保存到 data/final/train_segmented.txt。\n",
      "已保存到 data/final/val_segmented.txt。\n",
      "已保存到 data/final/test_segmented.txt。\n",
      "已保存到 data/final/train_index.txt。\n",
      "已保存到 data/final/val_index.txt。\n",
      "已保存到 data/final/test_index.txt。\n"
     ]
    }
   ],
   "source": [
    "def save_data(data: DataFrame, file_path: str, sep: str = ''):\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"{file_path} 已存在，跳过保存。\")\n",
    "        return\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        for X, y in data.values:\n",
    "            if not sep:\n",
    "                f.write(f\"{X}:{y}\\n\")\n",
    "            else:\n",
    "                f.write(f\"{sep.join(map(str, X))}:{y}\\n\")\n",
    "    print(f\"已保存到 {file_path}。\")\n",
    "\n",
    "\n",
    "# 保存清理后的数据\n",
    "save_data(train_raw_cleaned, 'data/final/train_raw.txt')\n",
    "save_data(val_raw_cleaned, 'data/final/val_raw.txt')\n",
    "save_data(test_raw_cleaned, 'data/final/test_raw.txt')\n",
    "\n",
    "save_data(train_seg_cleaned, 'data/final/train_segmented.txt', sep='<sp>')\n",
    "save_data(val_seg_cleaned, 'data/final/val_segmented.txt', sep='<sp>')\n",
    "save_data(test_seg_cleaned, 'data/final/test_segmented.txt', sep='<sp>')\n",
    "\n",
    "save_data(train_idx, 'data/final/train_index.txt', sep=',')\n",
    "save_data(val_idx, 'data/final/val_index.txt', sep=',')\n",
    "save_data(test_idx, 'data/final/test_index.txt', sep=',')\n",
    "\n",
    "save_vocab(vocab, 'data/final/vocab.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpllma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
