{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5a4fcf5",
   "metadata": {},
   "source": [
    "## 04 中文分词与词元化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed05ded3",
   "metadata": {},
   "source": [
    "首先，我们读取从 `03 Data Augmentation.ipynb` 中保存的增强数据集，并将其拆分为训练集、验证集和测试集。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c9050d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据: 391818 行\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "sentiment_polarity",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "e17e543e-d26c-481d-9fd1-5c49a5027d0e",
       "rows": [
        [
         "0",
         "空心菜是世界上最好吃的青菜",
         "1"
        ],
        [
         "1",
         "下雪了？好美的雪啊",
         "1"
        ],
        [
         "2",
         "哈哈哈娱乐圈文必须有的环节之上综艺打电话环节，必给男主打电话",
         "1"
        ],
        [
         "3",
         "亚森发热 发热",
         "0"
        ],
        [
         "4",
         "人随春好，春与人宜。",
         "1"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment_polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>空心菜是世界上最好吃的青菜</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>下雪了？好美的雪啊</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>哈哈哈娱乐圈文必须有的环节之上综艺打电话环节，必给男主打电话</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>亚森发热 发热</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>人随春好，春与人宜。</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             text  sentiment_polarity\n",
       "0                   空心菜是世界上最好吃的青菜                   1\n",
       "1                       下雪了？好美的雪啊                   1\n",
       "2  哈哈哈娱乐圈文必须有的环节之上综艺打电话环节，必给男主打电话                   1\n",
       "3                         亚森发热 发热                   0\n",
       "4                      人随春好，春与人宜。                   1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取数据\n",
    "df = pd.read_csv('data/weibo_augmented.csv', encoding='utf-8-sig')\n",
    "\n",
    "df = df[[\"text\", \"sentiment_polarity\"]]\n",
    "print(f\"数据: {len(df)} 行\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91d6c76",
   "metadata": {},
   "source": [
    "### 标签映射"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a21b28e",
   "metadata": {},
   "source": [
    "可以看到，数据集使用 `[-1, 0, 1]` 表示情感极性。\n",
    "\n",
    "后续训练模型时会发现，反向传播时使用的损失函数（`NLLLoss` 或 `CrossEntropyLoss`）要求样本的标签为 `0 ~num_classes-1` 之间的整数，因此需要将情感标签从原始的 `[-1, 0, 1]` 映射到 `[0, 1, 2]`。\n",
    "\n",
    "这里使用函数 `remap_labels` 来实现标签的映射。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72e3b49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "2    167778\n",
      "0    124334\n",
      "1     99706\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pandas import DataFrame\n",
    "\n",
    "def remap_labels(data: DataFrame, label_mapping={-1:0, 0:1, 1:2}):  #@save\n",
    "    if 2 in data[\"label\"].values:\n",
    "        return data\n",
    "    remapped_data = []\n",
    "    for tokens, old_label in data.values:\n",
    "        new_label = label_mapping.get(old_label, old_label)\n",
    "        remapped_data.append((tokens, new_label))\n",
    "    return DataFrame(remapped_data, columns=[\"text\", \"label\"])\n",
    "\n",
    "df = df.rename(columns={\"sentiment_polarity\": \"label\"})\n",
    "df = remap_labels(df)\n",
    "print(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f968908",
   "metadata": {},
   "source": [
    "### 划分数据集\n",
    "在机器学习任务中，通常需要将数据集划分为训练集、验证集和测试集，以便模型能够在不同的数据上进行训练和评估。\n",
    "\n",
    "训练集用于模型的训练，验证集用于调参和选择最佳模型，测试集用于最终评估模型的性能。\n",
    "\n",
    "`sklearn.model_selection` 模块提供了方便的函数 `train_test_split`，可以轻松地将数据集划分为不同的子集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb446988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(274272, 58773, 58773)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df[\"text\"]\n",
    "y = df[\"label\"]\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "train_raw = DataFrame({\"text\": X_train, \"label\": y_train})\n",
    "val_raw = DataFrame({\"text\": X_val, \"label\": y_val})\n",
    "test_raw = DataFrame({\"text\": X_test, \"label\": y_test})\n",
    "\n",
    "len(train_raw), len(val_raw), len(test_raw)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4aa3f11",
   "metadata": {},
   "source": [
    "接下来，我们要考虑文本数据的分词与词元化处理。\n",
    "\n",
    "\n",
    "分词（Word Segmentation）是中文自然语言处理中的基础任务，旨在将连续的汉字序列切分成有意义的词语单元。与英文等语言不同，中文文本中词语之间没有明确的空格分隔符，这使得分词成为中文 NLP 任务中的一个关键步骤。\n",
    "\n",
    "词元化（Tokenization）是将文本转换为模型可处理的数值形式的过程。对于基于词袋模型（Bag-of-Words）或词嵌入（Word Embeddings）的模型，词元化通常涉及将分词后的词语映射到唯一的整数索引，形成词汇表（Vocabulary）。\n",
    "\n",
    "下面我们将逐步完成分词、构建词汇表以及词元化的过程。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0479a8",
   "metadata": {},
   "source": [
    "### 分词（Word Segmentation）\n",
    "\n",
    "我们使用 LTP 作为分词工具。LTP（Language Technology Platform，语言技术平台）是哈尔滨工业大学社会计算与信息检索研究中心（HIT-SCIR）历时多年研发的一整套高效、高精度的中文自然语言处理开源基础技术平台。相较于常用的分词工具如 Jieba，LTP 在分词方面各方面的表现均较为优秀，适合用于对文本进行高质量的分词处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "689a9501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始处理训练集...\n",
      "文件 data/train_segmented.txt 已存在，直接读取。\n",
      "\n",
      "开始处理验证集...\n",
      "文件 data/val_segmented.txt 已存在，直接读取。\n",
      "\n",
      "开始处理测试集...\n",
      "文件 data/test_segmented.txt 已存在，直接读取。\n"
     ]
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "\n",
    "def segment_data(X, y, file_path='', batch_size=500):  #@save\n",
    "    \"\"\"\n",
    "    分批处理数据以避免内存不足问题\n",
    "    batch_size: 每批处理的数据量，默认1000条\n",
    "    \"\"\"\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"文件 {file_path} 已存在，直接读取。\")\n",
    "        return load_data(file_path, sep='<sp>', is_segmented=True)\n",
    "    if not isinstance(X, list):\n",
    "        X = X.tolist()\n",
    "    if not isinstance(y, list):\n",
    "        y = y.tolist()\n",
    "    \n",
    "    from ltp import LTP\n",
    "    ltp = LTP()\n",
    "\n",
    "    segmented_data = []\n",
    "    \n",
    "    print(f\"开始处理 {len(X)} 条数据，批大小: {batch_size}\")\n",
    "    \n",
    "    if len(file_path) > 0:\n",
    "        # 打开文件准备写入\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            # 分批处理\n",
    "            for i in range(0, len(X), batch_size):\n",
    "                batch_end = min(i + batch_size, len(X))\n",
    "                batch_X = X[i:batch_end]\n",
    "                batch_y = y[i:batch_end]\n",
    "                \n",
    "                print(f\"正在处理第 {i//batch_size + 1} 批，数据范围: {i}-{batch_end-1}\")\n",
    "                \n",
    "                # 对当前批次进行分词\n",
    "                segment = ltp.pipeline(batch_X, tasks=['cws'], return_dict=False)[0]\n",
    "                \n",
    "                # 写入文件\n",
    "                for sublist, label in zip(segment, batch_y):\n",
    "                    segmented_data.append((sublist, label))\n",
    "                    f.write('<sp>'.join(sublist) + ':' + str(label) + '\\n')\n",
    "                \n",
    "                print(f\"第 {i//batch_size + 1} 批处理完成\")\n",
    "    \n",
    "    print(\"所有数据处理完成！\")\n",
    "    return DataFrame(segmented_data, columns=[\"text\", \"label\"])\n",
    "\n",
    "def load_data(  #@save\n",
    "        file_path: str, \n",
    "        sep='', \n",
    "        is_segmented=False, \n",
    "        is_indexed=False) -> DataFrame:\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8-sig') as f:\n",
    "        for line in f:\n",
    "            X, y = line.rsplit(':', 1)\n",
    "            if is_segmented:\n",
    "                X = [token.strip() for token in X.split(sep) if token.strip() != '']\n",
    "            elif is_indexed:\n",
    "                X = list(map(int, X.split(sep)))\n",
    "            y = int(float(y))\n",
    "            data.append((X, y))\n",
    "    return DataFrame(data, columns=[\"text\", \"label\"])\n",
    "\n",
    "\n",
    "# 分批处理数据，使用较小的批大小以避免内存问题\n",
    "print(\"开始处理训练集...\")\n",
    "train_seg = segment_data(X_train, y_train, 'data/train_segmented.txt', batch_size=500)\n",
    "\n",
    "print(\"\\n开始处理验证集...\")\n",
    "val_seg = segment_data(X_val, y_val, 'data/val_segmented.txt', batch_size=500)\n",
    "\n",
    "print(\"\\n开始处理测试集...\")\n",
    "test_seg = segment_data(X_test, y_test, 'data/test_segmented.txt', batch_size=500);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcd457c",
   "metadata": {},
   "source": [
    "### 构建词汇表（Vocabulary）\n",
    "\n",
    "无论是使用深度学习还是使用传统的统计机器学习方法处理自然语言，都需要将输入的语言符号（通常为词元）映射为大于等于 0、小于词表大小的整数，该证书也被乘坐一个词元的索引值或下表。\n",
    "\n",
    "下面，我们亲自动手实现一个 `Vocab` 类，实现词元和索引之间的相互映射。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4159cbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self, tokens=None) -> None:\n",
    "        self.idx_to_token = list()\n",
    "        self.token_to_idx = dict()\n",
    "\n",
    "        if tokens is not None:\n",
    "            if \"<unk>\" not in tokens:\n",
    "                tokens = tokens + [\"<unk\"]\n",
    "            for token in tokens:\n",
    "                self.idx_to_token.append(token)\n",
    "                self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
    "            self.unk = self.token_to_idx[\"<unk>\"]\n",
    "\n",
    "    @classmethod\n",
    "    def build(cls, text, min_freq=1, reserved_tokens=None):\n",
    "        token_freqs = defaultdict(int)\n",
    "        for sentence in text:\n",
    "            for token in sentence:\n",
    "                token_freqs[token] += 1\n",
    "        \n",
    "        uniq_tokens = [\"<unk>\"] + (reserved_tokens if reserved_tokens else [])\n",
    "        uniq_tokens += [token for token, freq in token_freqs.items() \n",
    "                        if freq >= min_freq and token != \"<unk>\"]\n",
    "        return cls(uniq_tokens)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.idx_to_token)\n",
    "    \n",
    "    def __getitem__(self, token):\n",
    "        \"\"\"查找输入词元对应的索引值，若不存在，则返回<unk>的索引值（0）\"\"\"\n",
    "        return self.token_to_idx.get(token, self.unk)\n",
    "    \n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        \"\"\"查找一系列输入词元的索引值\"\"\"\n",
    "        return [self[token] for token in tokens]\n",
    "    \n",
    "    def convert_ids_to_tokens(self, indices):\n",
    "        \"\"\"查找一系列输入索引值对应的词元\"\"\"\n",
    "        return [self.idx_to_token[index] for index in indices]\n",
    "\n",
    "def save_vocab(vocab: Vocab, file_path: str):\n",
    "    with open(file_path, 'w', encoding='utf-8-sig') as f:\n",
    "        f.write('\\n'.join(vocab.idx_to_token))\n",
    "\n",
    "def read_vocab(file_path: str) -> Vocab:\n",
    "    with open(file_path, 'r', encoding='utf-8-sig') as f:\n",
    "        tokens = f.read().split('\\n')\n",
    "    return Vocab(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abb1e16",
   "metadata": {},
   "source": [
    "在实例化 `Vocab` 类创建词汇表对象前，我们需要解决两个重要的问题：\n",
    "\n",
    "1. 应该在什么数据集上构建词汇表？\n",
    "2. 如何设定类方法 `build` 中的最小词频参数 `min_freq`？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add34dad",
   "metadata": {},
   "source": [
    "首先来看第1个问题。问题换言之，我们是应该使用训练集、验证集和测试集组成的整个大数据集上构建词汇表，还是仅使用训练集来构建词汇表？\n",
    "\n",
    "在这里，我们采用仅使用训练集来构建词汇表的策略。原因有二：\n",
    "- **防止数据泄漏（Data Leakage）**：使用验证集和测试集的数据来构建词汇表可能会导致模型在训练过程中“看到”这些数据，从而影响模型的泛化能力。为了确保模型的评估结果真实反映其在未见数据上的表现，应该避免在训练过程中使用验证集和测试集的信息。\n",
    "- **模拟真实应用场景**：在实际应用中，模型通常只能访问训练数据，而无法预先了解未来的验证或测试数据。因此，仅使用训练集构建词汇表更符合实际应用场景，有助于提高模型在真实环境中的表现。\n",
    "\n",
    "尽管也有一些研究称，同时使用验证集和测试集构建词表并不会导致数据泄露（因为词表作为统计资源，并不包含标签信息或特征分布信息），但为了尽可能模拟现实的应用场景，以及作为工业上的更常用策略，我们仍然仅使用训练集来构建词汇表。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df18d0b4",
   "metadata": {},
   "source": [
    "接着是第2个问题，即如何设定最小词频参数 `min_freq`？\n",
    "\n",
    "最小词频参数 `min_freq` 是构建词表时用来过滤低频词的重要参数，它将直接影响词表大小、模型的泛化能力和训练速度等。\n",
    "\n",
    "具体而言，`min_freq` 的作用在于：只有当某个词出现次数大于等于 `min_freq` 时，才会被保留到词表中；否则将视为“低频词”，使用 `<unk>`（未知词）进行替代。\n",
    "\n",
    "如果 `min_freq` 设置太小，优点在于能够最大程度保留词汇信息，缺点在于会导致词表过大，进而导致模型训练较慢，并且带来较大噪声；\n",
    "\n",
    "相反，如果 `min_freq` 设置过大，优点在于能够显著减少词表大小，提高训练速度，缺点在于会丢失太多低频但重要的词，影响情感分类准确率。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc3e07f",
   "metadata": {},
   "source": [
    "关于 `min_freq` 的设置，可以参考下面的经验标准：\n",
    "|数据量|推荐 `min_freq`|\n",
    "|-|-|\n",
    "|<1 万条文本|1|\n",
    "|1~10 万条文本|2~3|\n",
    "|10~100 万条文本|3~5|\n",
    "|>100万条文本|5~10|\n",
    "\n",
    "在前面的部分，我们已知训练集的大小约为 27 万条文本，因此推荐 `min_freq` 设置在 3~5 之间。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316a8a22",
   "metadata": {},
   "source": [
    "进一步地，我们可以通过观察词频分布与统计覆盖率的方法，更加科学与定量地确定 `min_freq` 的值。\n",
    "\n",
    "首先，我们统计训练集中各词的词频分布情况，并绘制词频分布图。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3d82aea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "token_counts = Counter(token for text in train_seg[\"text\"] for token in text)\n",
    "freqs = list(token_counts.values())\n",
    "plt.hist(freqs, bins=100, range=(0,50))\n",
    "plt.xlabel('词元数量')\n",
    "plt.ylabel('词频')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7039b6c",
   "metadata": {},
   "source": [
    "通过观察图中词频与词汇数量的关系，我们可以发现大部分词的词频较低，而少数高频词占据了大部分文本内容。这符合中文微博语料的长尾分布特性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9b64a6",
   "metadata": {},
   "source": [
    "我们可以查看频率最高的前50个词元："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc8cefd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('的', 145046),\n",
       " ('了', 106278),\n",
       " ('，', 94144),\n",
       " ('我', 91002),\n",
       " ('是', 47335),\n",
       " ('不', 44673),\n",
       " ('。', 44448),\n",
       " ('！', 40212),\n",
       " ('一', 36954),\n",
       " ('好', 32228),\n",
       " ('啊', 32084),\n",
       " ('在', 23140),\n",
       " ('就', 23030),\n",
       " ('都', 22217),\n",
       " ('想', 19811),\n",
       " ('你', 18948),\n",
       " ('有', 17781),\n",
       " ('要', 16599),\n",
       " ('吃', 16493),\n",
       " ('人', 16370),\n",
       " ('天', 15765),\n",
       " ('到', 15576),\n",
       " ('这', 14850),\n",
       " ('个', 14716),\n",
       " ('也', 14252),\n",
       " ('看', 13739),\n",
       " ('又', 13601),\n",
       " ('很', 13468),\n",
       " ('能', 13388),\n",
       " ('没', 12484),\n",
       " ('和', 12422),\n",
       " ('会', 12070),\n",
       " ('今天', 11830),\n",
       " ('一个', 11277),\n",
       " ('小', 11110),\n",
       " ('去', 11044),\n",
       " ('真', 11027),\n",
       " (',', 10775),\n",
       " ('太', 10658),\n",
       " ('上', 10623),\n",
       " ('还', 10612),\n",
       " ('自己', 10490),\n",
       " ('来', 10016),\n",
       " ('着', 8863),\n",
       " ('多', 8168),\n",
       " ('给', 8079),\n",
       " ('说', 7700),\n",
       " ('得', 7552),\n",
       " ('这个', 7503),\n",
       " ('爱', 7313)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_counts.most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9321b2b9",
   "metadata": {},
   "source": [
    "对高频词稍作分析：\n",
    "- 出现标点符号：如 `，`、`。`、`！` 等\n",
    "    - 对于分句作用的符号，如 `，`、`。`、`、` 等，对于情感表达的作用不大，可以考虑去除。\n",
    "    - 对于表示情感的符号，如 `！`、`？` 等，可以考虑保留。\n",
    "- 出现虚词，如 `的`、`了`、`在`\n",
    "    - 尽管此类词语在语义上作用不大，但：\n",
    "        - 对传统机器学习来说，可通过权重自动弱化\n",
    "        - 对深度模型来说，它们的词嵌入会自动学到较弱权重\n",
    "    - 因此考虑保留此类词语"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b66a7db",
   "metadata": {},
   "source": [
    "接着，查看词元计数器中出现的所有标点符号："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "269e4c15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('，', 94144),\n",
       " ('。', 44448),\n",
       " ('(', 2942),\n",
       " (')', 2932),\n",
       " ('！', 40212),\n",
       " ('？', 7211),\n",
       " ('_', 323),\n",
       " (',', 10775),\n",
       " ('!', 395),\n",
       " ('（', 4466),\n",
       " ('）', 3302),\n",
       " ('?', 2078),\n",
       " ('：', 5391),\n",
       " ('@', 1),\n",
       " (':', 450),\n",
       " ('#', 77),\n",
       " ('##', 3),\n",
       " ('###', 3),\n",
       " ('、', 583),\n",
       " ('\"', 392),\n",
       " ('【', 502),\n",
       " ('】', 480),\n",
       " ('；', 174),\n",
       " ('...', 38),\n",
       " ('_______', 2),\n",
       " ('.', 59),\n",
       " ('《', 45),\n",
       " ('》', 45),\n",
       " ('___', 5),\n",
       " ('“', 36),\n",
       " ('”', 30),\n",
       " ('*', 1),\n",
       " ('[', 7),\n",
       " (']', 7),\n",
       " ('____', 3),\n",
       " (';', 15),\n",
       " ('§', 7),\n",
       " ('¶', 3),\n",
       " ('-', 37),\n",
       " ('\\\\', 1),\n",
       " ('__', 5),\n",
       " ('__________', 1),\n",
       " ('____________', 1),\n",
       " ('%', 2),\n",
       " (\"'\", 1),\n",
       " ('______', 1),\n",
       " ('___________', 1),\n",
       " ('（）', 1),\n",
       " ('..', 1)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unicodedata\n",
    "\n",
    "punct_tokens = [\n",
    "    (t, freq) for t, freq in token_counts.items() \n",
    "    if all(unicodedata.category(ch).startswith('P') \n",
    "           for ch in t)]\n",
    "\n",
    "punct_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d71dcf1",
   "metadata": {},
   "source": [
    "对标点符号的进一步分类与分析：\n",
    "- 表示情感的符号：如 `！`、`？` 等，保留\n",
    "\n",
    "- 分句符号：如 `，`、`。`、`；` 等，去除\n",
    "\n",
    "- 结构性符号：如 `（`、`）`、`【`、`】` 等，去除\n",
    "\n",
    "- 异常符号（人工添加线）：如 `_`、`__` 等，去除\n",
    "\n",
    "因此，需要对一些标点符号进行过滤，去除不必要的标点符号。\n",
    "\n",
    "在清洗分词数据集的同时，也需要将未分词的原始数据集中对应的数据一并清洗，以保证分词数据集与原始数据集的一一对应关系。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e98a7c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始数据长度:\n",
      "train: seg=274272, raw=274272\n",
      "val: seg=58773, raw=58773\n",
      "test: seg=58773, raw=58773\n",
      "\n",
      "开始同步清理...\n",
      "开始清理数据，原始长度: seg=274272, raw=274272\n",
      "清理完成，删除了 1249 个样本\n",
      "清理后长度: seg=273023, raw=273023\n",
      "开始清理数据，原始长度: seg=58773, raw=58773\n",
      "清理完成，删除了 304 个样本\n",
      "清理后长度: seg=58469, raw=58469\n",
      "开始清理数据，原始长度: seg=58773, raw=58773\n",
      "清理完成，删除了 276 个样本\n",
      "清理后长度: seg=58497, raw=58497\n",
      "\n",
      "最终数据长度:\n",
      "train: seg=273023, raw=273023\n",
      "val: seg=58469, raw=58469\n",
      "test: seg=58497, raw=58497\n"
     ]
    }
   ],
   "source": [
    "def clean_punct_synchronized(seg_data: DataFrame, raw_data: DataFrame):\n",
    "    \"\"\"同步清理分词数据和原始数据，确保删除的是相同的样本\"\"\"\n",
    "    \n",
    "    remove_punct = {'，', ',', '。', '；', ';',  '：', ':',\n",
    "                     '\"', '\"', '\"', '（', '）', '(', ')',  \n",
    "                     '【', '】', '[', ']',  '《', '》', \n",
    "                     '#', '_'}\n",
    "    \n",
    "    cleaned_seg_data = []\n",
    "    cleaned_raw_data = []\n",
    "    removed_indices = []\n",
    "    \n",
    "    print(f\"开始清理数据，原始长度: seg={len(seg_data)}, raw={len(raw_data)}\")\n",
    "    \n",
    "    for i in range(len(seg_data)):\n",
    "        seg_tokens = seg_data.iloc[i]['text']\n",
    "        seg_label = seg_data.iloc[i]['label']\n",
    "        raw_text = raw_data.iloc[i]['text'] \n",
    "        raw_label = raw_data.iloc[i]['label']\n",
    "        \n",
    "        # 清理分词数据中的标点符号\n",
    "        cleaned_tokens = [token for token in seg_tokens if token not in remove_punct]\n",
    "        \n",
    "        # 检查清理后的文本是否过短\n",
    "        if len(''.join(cleaned_tokens)) <= 3:\n",
    "            removed_indices.append(i)\n",
    "        else:\n",
    "            cleaned_seg_data.append([cleaned_tokens, seg_label])\n",
    "            cleaned_raw_data.append([raw_text, raw_label])\n",
    "    \n",
    "    cleaned_seg_df = pd.DataFrame(cleaned_seg_data, columns=[\"text\", \"label\"])\n",
    "    cleaned_raw_df = pd.DataFrame(cleaned_raw_data, columns=[\"text\", \"label\"])\n",
    "    \n",
    "    print(f\"清理完成，删除了 {len(removed_indices)} 个样本\")\n",
    "    print(f\"清理后长度: seg={len(cleaned_seg_df)}, raw={len(cleaned_raw_df)}\")\n",
    "    \n",
    "    return cleaned_seg_df, cleaned_raw_df, removed_indices\n",
    "\n",
    "\n",
    "print(\"原始数据长度:\")\n",
    "print(f\"train: seg={len(train_seg)}, raw={len(train_raw)}\")\n",
    "print(f\"val: seg={len(val_seg)}, raw={len(val_raw)}\")\n",
    "print(f\"test: seg={len(test_seg)}, raw={len(test_raw)}\")\n",
    "\n",
    "# 使用正确的方法同步清理数据\n",
    "print(\"\\n开始同步清理...\")\n",
    "train_seg_cleaned, train_raw_cleaned, train_removed_indices = clean_punct_synchronized(train_seg, train_raw)\n",
    "val_seg_cleaned, val_raw_cleaned, val_removed_indices = clean_punct_synchronized(val_seg, val_raw)\n",
    "test_seg_cleaned, test_raw_cleaned, test_removed_indices = clean_punct_synchronized(test_seg, test_raw)\n",
    "\n",
    "\n",
    "print(f\"\\n最终数据长度:\")\n",
    "print(f\"train: seg={len(train_seg_cleaned)}, raw={len(train_raw_cleaned)}\")\n",
    "print(f\"val: seg={len(val_seg_cleaned)}, raw={len(val_raw_cleaned)}\")\n",
    "print(f\"test: seg={len(test_seg_cleaned)}, raw={len(test_raw_cleaned)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc667e54",
   "metadata": {},
   "source": [
    "清除掉一些低质量数据后，我们开始正式对 `min_freq` 的设置进行分析，使用的方法为：统计覆盖率（Coverage）。\n",
    "\n",
    "覆盖率是指词表中包含的词元所覆盖的文本总词数占文本总词数的比例。通过计算不同 `min_freq` 值下的覆盖率，我们可以评估不同设置对文本表示能力的影响。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "530059c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_freq: 1\tcoverage: 1.0\n",
      "min_freq: 2\tcoverage: 0.9771739629065274\n",
      "min_freq: 3\tcoverage: 0.9679520480263639\n",
      "min_freq: 5\tcoverage: 0.9559290291944649\n",
      "min_freq: 10\tcoverage: 0.9369960833518276\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "token_counts = Counter(token for sentence in train_seg[\"text\"] for token in sentence)\n",
    "total_tokens = sum(token_counts.values())\n",
    "\n",
    "for mf in [1, 2, 3, 5, 10]:\n",
    "    kept_tokens = sum(count for count in token_counts.values() if count >= mf)\n",
    "    print(f\"min_freq: {mf}\\tcoverage: {kept_tokens / total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73ad6a6",
   "metadata": {},
   "source": [
    "一般来说，当覆盖率 `coverage` 不小于 95% 时，过滤效果比较合理。\n",
    "\n",
    "因此，构建词汇表时，我们设置最小词频 `min_freq=3`，即词频小于 3 的词元将被视为 `<UNK>` 词元，最终可达到约 96.8% 的覆盖率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d43d6e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab.build(train_seg_cleaned[\"text\"], min_freq=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7adf0484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35272"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5f32f7",
   "metadata": {},
   "source": [
    "可以看到，最终构建的词汇表大小为 35,272 个词元。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d933bdb3",
   "metadata": {},
   "source": [
    "### 词元化（Tokenization）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dafae0",
   "metadata": {},
   "source": [
    "构建好词汇表后，就可以进行词元化处理。具体来说，对数据集中每个句子使用 `vocab.convert_tokens_to_ids` 方法，将句子中的每个词元转换为词元在词汇表中的索引序列，供后续模型训练使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4df8da36",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tok = DataFrame([\n",
    "    [vocab.convert_tokens_to_ids(text), label]\n",
    "    for text, label in train_seg_cleaned.values\n",
    "], columns=[\"text\", \"label\"])\n",
    "\n",
    "val_tok = DataFrame([\n",
    "    [vocab.convert_tokens_to_ids(text), label]\n",
    "    for text, label in val_seg_cleaned.values\n",
    "], columns=[\"text\", \"label\"])\n",
    "\n",
    "test_tok = DataFrame([\n",
    "    [vocab.convert_tokens_to_ids(text), label]\n",
    "    for text, label in test_seg_cleaned.values\n",
    "], columns=[\"text\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4251d04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样本 53301：\n",
      "原始句子： ['谁', '把', '篓子', '捅', '到', '了', '省', '院']\n",
      "词元索引： [122, 369, 20040, 6426, 16, 31, 14500, 3972]\n",
      "\n",
      "样本 270658：\n",
      "原始句子： ['我', '去', '过', '重庆', '第868', '次']\n",
      "词元索引： [54, 281, 689, 3658, 0, 49]\n",
      "\n",
      "样本 227322：\n",
      "原始句子： ['强大', '心理', '强大', '心理', '强大', '心理']\n",
      "词元索引： [1089, 3755, 1089, 3755, 1089, 3755]\n",
      "\n",
      "样本 28392：\n",
      "原始句子： ['羡慕', '的', '想', '哭', '请', '老天', '赐', '我', '一个', '高大', '英俊', '的', '男朋友', '好', '吗']\n",
      "词元索引： [6892, 11, 20, 1816, 373, 555, 18194, 54, 297, 20952, 20953, 11, 1277, 42, 69]\n",
      "\n",
      "样本 152469：\n",
      "原始句子： ['快快好', '起来']\n",
      "词元索引： [15610, 514]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "for i in random.sample(range(len(train_seg)), 5):\n",
    "    print(f\"样本 {i+1}：\")\n",
    "    print(\"原始句子：\", train_seg_cleaned['text'].iloc[i])\n",
    "    print(\"词元索引：\", train_tok['text'].iloc[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d68eb4",
   "metadata": {},
   "source": [
    "将数据集与词汇表保存至本地。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93be4b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/train_raw_final.txt 已存在，跳过保存。\n",
      "data/val_raw_final.txt 已存在，跳过保存。\n",
      "data/test_raw_final.txt 已存在，跳过保存。\n",
      "data/train_segmented_final.txt 已存在，跳过保存。\n",
      "data/val_segmented_final.txt 已存在，跳过保存。\n",
      "data/test_segmented_final.txt 已存在，跳过保存。\n",
      "data/train_tokenized_final.txt 已存在，跳过保存。\n",
      "data/val_tokenized_final.txt 已存在，跳过保存。\n",
      "data/test_tokenized_final.txt 已存在，跳过保存。\n"
     ]
    }
   ],
   "source": [
    "def save_data(data: DataFrame, file_path: str, sep: str = ''):  #@save\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"{file_path} 已存在，跳过保存。\")\n",
    "        return\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        for X, y in data.values:\n",
    "            if not sep:\n",
    "                f.write(f\"{X}:{y}\\n\")\n",
    "            else:\n",
    "                f.write(f\"{sep.join(map(str, X))}:{y}\\n\")\n",
    "    print(f\"已保存到 {file_path}。\")\n",
    "\n",
    "\n",
    "# 保存清理后的数据\n",
    "save_data(train_raw_cleaned, 'data/train_raw_final.txt')\n",
    "save_data(val_raw_cleaned, 'data/val_raw_final.txt')\n",
    "save_data(test_raw_cleaned, 'data/test_raw_final.txt')\n",
    "\n",
    "save_data(train_seg_cleaned, 'data/train_segmented_final.txt', sep='<sp>')\n",
    "save_data(val_seg_cleaned, 'data/val_segmented_final.txt', sep='<sp>')\n",
    "save_data(test_seg_cleaned, 'data/test_segmented_final.txt', sep='<sp>')\n",
    "\n",
    "\n",
    "save_data(train_tok, 'data/train_tokenized_final.txt', sep=',')\n",
    "save_data(val_tok, 'data/val_tokenized_final.txt', sep=',')\n",
    "save_data(test_tok, 'data/test_tokenized_final.txt', sep=',')\n",
    "\n",
    "save_vocab(vocab, 'data/vocab_final.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpllma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
